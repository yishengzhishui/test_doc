## 1.数组是可以比较的，切片是不可比较的。

因为切片是引用类型，它包含指向底层数组的指针、长度和容量。两个切片即使引用相同类型的底层数组，它们也不会被认为是相等的，因为它们的指针、长度和容量可能不同。

切片未初始化，也是可以插入的。

无需显式的初始化：

```go
var i int         // i 的零值为 0
var f float64     // f 的零值为 0.0
var s string      // s 的零值为 ""
var b bool        // b 的零值为 false
var ptr *int      // ptr 的零值为 nil
var arr [3]int    // arr 的零值为 [0 0 0]
var complexNum complex128 // complexNum 的零值为 (0+0i)
```

## 2.TCP的三次握手和四次挥手

TCP（传输控制协议）的三次握手和四次挥手是建立和断开连接的过程，确保数据的可靠传输。以下是对TCP三次握手和四次挥手的详细解释：

### TCP的三次握手（Three-Way Handshake）：

1. **第一步 - 客户端发送SYN（同步）请求：**

   - 客户端向服务器发送一个TCP报文，标志位设置为SYN（同步），并选择一个初始序列号（ISN）。
2. **第二步 - 服务器确认SYN并发送自己的SYN请求：**

   - 服务器收到客户端的SYN请求后，向客户端发送一个TCP报文，标志位设置为SYN和ACK（确认），同时选择自己的初始序列号。
3. **第三步 - 客户端确认服务器的SYN请求：**

   - 客户端收到服务器的SYN和ACK后，向服务器发送一个TCP报文，标志位设置为ACK，确认连接建立。

### TCP的四次挥手（Four-Way Handshake）：

1. **第一步 - 客户端发起关闭：**

   - 客户端向服务器发送一个TCP报文，标志位设置为FIN（结束）。
2. **第二步 - 服务器确认关闭请求：**

   - 服务器收到客户端的FIN后，发送一个TCP报文，标志位设置为ACK，确认收到关闭请求。
3. **第三步 - 服务器发起关闭：**

   - 服务器向客户端发送一个TCP报文，标志位设置为FIN，表示服务器准备关闭连接。
4. **第四步 - 客户端确认关闭：**

   - 客户端收到服务器的FIN后，发送一个TCP报文，标志位设置为ACK，表示客户端确认关闭。

通过这个过程，TCP连接被优雅地关闭。三次握手确保连接的建立，而四次挥手确保连接的安全关闭，双方都有机会完成未完成的数据传输。这种连接的建立和关闭过程是为了确保数据的可靠传输，以及释放网络资源。

## 3.事物的ACID

ACID 是数据库事务的四个关键属性，用于确保在事务执行过程中数据的一致性和可靠性。ACID 表示如下：

1. **原子性（Atomicity）：**

   - 原子性要求事务是不可分割的操作单元，要么完全执行，要么完全不执行。如果事务的任何部分失败，整个事务都会被回滚到原始状态，不会产生部分完成的结果。
2. **一致性（Consistency）：**

   - 一致性确保事务执行后，数据库从一个一致的状态转移到另一个一致的状态。事务的执行不能破坏数据库的完整性约束，例如唯一键、外键等。即使在事务执行期间发生错误，数据库也应该恢复到一致的状态。
3. **隔离性（Isolation）：**

   - 隔离性定义了事务之间的独立性，即一个事务的执行不应该受到其他并发事务的影响。事务的隔离性防止了并发执行时可能发生的问题，如脏读、不可重复读和幻读。数据库系统通过实现不同的隔离级别来提供不同程度的隔离性。
4. **持久性（Durability）：**

   - 持久性确保一旦事务被提交，其结果就会永久保存在数据库中，即使系统发生故障或崩溃。数据库系统通过将事务的日志记录到稳定的存储介质（如磁盘）来实现持久性。

这四个属性共同确保了事务的可靠性和稳定性。ACID 属性是数据库系统中保障数据一致性和完整性的基本原则，对于涉及关键业务数据的应用非常重要。

## 4.跨域问题

协议、域名和端口，任意一个不同就是跨域请求

在 Gin 框架中，要使用 CORS 中间件来解决跨域问题，你可以使用 `github.com/gin-contrib/cors` 包。设置`AllowHeaders`、`AllowOrigin`等。

```go
func corsHandle() gin.HandlerFunc {
	return cors.New(cors.Config{
		AllowHeaders: []string{"Content-Type", "Authorization"},
		// 你不加这个，前端是拿不到的
		ExposeHeaders: []string{"x-jwt-token", "x-refresh-token"},
		// 是否允许你带 cookie 之类的东西
		AllowCredentials: true,
		AllowOriginFunc: func(origin string) bool {
			if strings.HasPrefix(origin, "http://localhost") {
				// 你的开发环境
				return true
			}
			return strings.Contains(origin, "yourcompany.com")
		},
		MaxAge: 12 * time.Hour,
	})
}

```

## 5.Cookie 和Session

Cookie 是一种小型的文本文件，由服务器发送给用户的浏览器，并存储在用户的计算机上。

它包含了与用户相关的信息，比如用户的身份认证、会话数据等。Cookie 主要用于在浏览器和服务器之间保持状态，以实现跟踪用户行为、记录用户偏好等功能。当用户访问同一站点时，浏览器会将相关的 Cookie 信息附加到请求头中，从而向服务器提供存储在本地的信息。

Session 是服务器端用于存储用户信息的一种机制。与 Cookie 不同，Session 数据存储在服务器上，而不是在用户的浏览器中。当用户访问服务器时，服务器会为每个用户创建一个唯一的 Session，然后将 Session ID 发送给客户端，通常通过 Cookie 进行存储。客户端的 Cookie 中包含了 Session ID，而实际的用户数据则存储在服务器上。

## 6.go的recover

在 Go 语言中，`recover` 是一个内建函数，用于捕获在 `defer` 语句中由 `panic` 引发的运行时恐慌。`recover` 可以阻止 panic 继续传播，允许程序继续执行而不中断。可以实现类似php的try catch。

在 Go 中，可以通过使用 `recover` 来捕获 `panic`，从而避免程序因 `panic` 而崩溃。但需要注意的是，`recover` 只有在 `defer` 函数中调用才有效。

## 7.GO的GMP模型

Go 的 GMP 模型是指 Goroutine（协程）、M（线程管理器）、P（处理器）这三者之间的协作模型。这个模型是 Go 语言调度器实现并发的基础。以下是 GMP 模型的简述：

1. **Goroutine（协程）：**

   - Goroutine 是 Go 语言并发的基本单位，它是轻量级的用户态线程。
   - 每个 Goroutine 都有一个独立的执行栈，占用的内存相对较小。
   - 在程序执行过程中，可以创建成千上万个 Goroutine，而不会显著影响系统性能。
2. **M（线程管理器）：**

   - M 是 Go 语言中的线程管理器，负责管理和调度 Goroutine。
   - 在多核机器上，M 会与物理处理器一一对应。一个物理处理器上可以有多个 M，每个 M 负责管理一组 Goroutine。
   - M 会在需要的时候创建或销毁内核线程，以确保 Goroutine 能够被适当地并发执行。
3. **P（处理器）：**

   - P 是调度器的逻辑处理器，用于将 Goroutine 分配给 M 执行。
   - P 的数量可以在运行时调整，以适应系统的负载。
   - P 会维护一个 Goroutine 队列，当某个 M 空闲时，会从队列中获取 Goroutine 分配给 M 执行。
4. **GMP 之间的关系：**

   - M 和 P 是多对多的关系，即一个 M 可以和多个 P 关联，一个 P 也可以和多个 M 关联。
   - 当一个 Goroutine 被创建时，调度器会将其放入某个 P 的队列中。
   - 当一个 M 变得空闲时，它会从与之关联的 P 的队列中获取 Goroutine 执行。

GMP 模型的优势在于它充分利用了多核处理器的性能，并且在 Goroutine 的调度上实现了高度的灵活性。这种模型使得 Go 语言能够在高并发的情况下高效地运行，并且对于并发编程提供了简单而强大的抽象。

## 8.click house数据存储结构及主要特点

ClickHouse 是一个开源的列式数据库管理系统，它采用了列式存储结构，这种存储结构与传统的行式数据库存储结构有所不同。以下是 ClickHouse 数据存储结构的主要特点：

1. **列式存储：**

   - ClickHouse 是一种列式存储数据库，数据按列存储而非按行存储。每个列都存储一个属性的数据，相同列的数据是连续存储的。
   - 这种存储方式在分析性质的查询中表现出色，因为它允许跳过不必要的列，只读取需要的列，从而提高查询性能。
2. **数据压缩：**

   - ClickHouse 使用了多种压缩算法，以减小存储占用和提高查询性能。常见的压缩算法包括 LZ4、ZSTD、Delta、T64 等。
   - 压缩有助于减小磁盘 I/O、提高数据传输效率，并降低存储成本。
3. **MergeTree 表引擎：**

   - ClickHouse 中最常用的表引擎是 MergeTree，它适用于处理大量的时间序列数据。
   - MergeTree 表引擎支持按照指定的列对数据进行排序，并定期合并小的块以提高查询性能。
4. **数据划分：**

   - ClickHouse 支持水平分区和垂直切分，以便更好地管理大规模的数据。
   - 水平分区允许将表分成若干部分，每个部分称为分区，每个分区可以单独存储在不同的服务器上。
   - 垂直切分则是将一张表按列进行分割，将不同的列存储在不同的表中，以提高查询效率。
5. **BloomFilter：**

   - ClickHouse 使用 BloomFilter 用于快速过滤掉不存在于某个条件下的数据，以减小查询的范围。

这些特点使 ClickHouse 在大规模数据分析和查询场景中表现优越，特别适用于需要快速查询大量历史数据的应用。了解 ClickHouse 的存储结构对于进行性能调优、数据模型设计以及数据仓库的搭建都是重要的。

## 9.clickhouse vs mysql

ClickHouse 和 MySQL 是两种不同类型的数据库系统，它们在很多方面有着不同的设计目标和应用场景。以下是 ClickHouse 和 MySQL 的一些对比：

1. **数据模型：**

   - **ClickHouse：** ClickHouse 是一种列式存储数据库，适用于分析型查询。它以高性能和高压缩比为目标，特别擅长处理大量数据的分析和聚合操作。
   - **MySQL：** MySQL 是一种行式存储数据库，适用于事务性应用和联机事务处理（OLTP）场景。它更适合处理频繁的读写操作。
2. **查询性能：**

   - **ClickHouse：** ClickHouse 在大规模数据分析查询上表现出色，可以在秒级别的时间内完成复杂的聚合查询。但对于事务性操作，性能可能相对较低。
   - **MySQL：** MySQL 在事务性操作上表现良好，适用于需要频繁进行插入、更新和删除操作的应用。但在大规模数据分析场景下，性能可能不如 ClickHouse。
3. **索引：**

   - **ClickHouse：** ClickHouse 的索引设计主要基于 MergeTree 引擎，支持对列的多级索引，适用于大规模数据的范围查询。
   - **MySQL：** MySQL 支持多种索引类型，包括 B-Tree、Hash、Full-Text 等，可以更灵活地满足不同查询需求。
4. **事务支持：**

   - **ClickHouse：** ClickHouse 对事务支持有限，通常用于批处理和分析型查询，不适合处理复杂的事务操作。
   - **MySQL：** MySQL 提供强大的事务支持，可以满足需要 ACID 特性的应用场景，例如电子商务和金融领域。
5. **数据压缩：**

   - **ClickHouse：** ClickHouse 使用列式存储和多种压缩算法，以减小存储占用空间，并提高查询性能。
   - **MySQL：** MySQL 也支持数据压缩，但通常在 InnoDB 存储引擎中使用，以节省磁盘空间。
6. **分布式架构：**

   - **ClickHouse：** ClickHouse 提供了分布式架构，可以横向扩展以处理更大规模的数据。
   - **MySQL：** MySQL 也支持主从复制和分片，但在处理大规模数据时需要谨慎设计和管理。
7. **社区和生态系统：**

   - **ClickHouse：** ClickHouse 的社区相对较小，但在大数据分析领域逐渐得到了认可。
   - **MySQL：** MySQL 有着庞大的社区和生态系统，广泛用于各种应用场景。

综合考虑业务需求，选择 ClickHouse 还是 MySQL 取决于具体的使用场景和性能要求。 ClickHouse 更适合大规模数据分析，而 MySQL 更适合事务性应用。

10. redis 异步延时队列

Redis 延迟队列是一种利用 Redis 数据库实现的队列，其中的消息在一定的延迟时间之后才会被消费。延迟队列常用于处理具有延迟触发需求的任务，例如定时任务、重试机制等。

以下是实现 Redis 延迟队列的一种常见方式：

1. **使用有序集合（Sorted Set）：**

   - Redis 的有序集合是一个有序的字符串集合，其中每个字符串都关联着一个分数。延迟队列可以使用有序集合来存储消息，其中消息的分数表示消息的到期时间。
2. **将消息加入有序集合：**

   - 当有一个新的消息要加入队列时，将消息的内容作为字符串存储在有序集合中，同时使用当前的时间戳加上延迟时间作为消息的分数。这样，消息就会根据到期时间被添加到有序集合中。

   ```bash
   ZADD delay_queue <timestamp + delay> <message>
   ```
3. **定时轮询获取到期消息：**

   - 客户端或者后台任务可以定时轮询有序集合，查找到期的消息。通过比较当前时间戳和消息的到期时间，可以找到需要处理的消息。

   ```bash
   ZRANGEBYSCORE delay_queue 0 <current_timestamp> WITHSCORES
   ```
4. **处理到期消息：**

   - 获取到期的消息后，进行相应的处理，可以是执行任务、触发事件等。处理完成后，将消息从有序集合中移除。

   ```bash
   ZREM delay_queue <message>
   ```

通过上述方式，可以实现一个简单的 Redis 延迟队列。需要注意的是，这是一种基本的实现方式，具体根据业务需求和性能要求可以进行更复杂的优化和扩展。例如，可以考虑使用 Lua 脚本来原子地处理添加和移除操作，以保证操作的一致性。

## 10.进程、线程和协程

进程（Process）、线程（Thread）和协程（Coroutine）是计算机科学中用于实现并发执行的三种基本的执行单元。

**线程是独立调度的基本单位，进程是资源拥有的基本单位** 。

它们有着不同的特性和用途：

1. **进程（Process）：**

   - 进程是操作系统中的一个独立的执行环境。每个进程都有自己独立的地址空间、内存、文件描述符等资源，进程之间通常通过进程间通信（Inter-Process Communication，IPC）来进行数据交换。由于进程拥有独立的资源，进程之间的隔离性很强，但进程的创建和切换开销相对较大。
2. **线程（Thread）：**

   - 线程是进程中的一个执行单元，多个线程共享同一个进程的资源，包括地址空间、文件描述符等。线程的创建和切换开销相对较小，因为它们共享进程的资源。但多线程之间需要考虑同步和互斥，以避免竞态条件和数据访问冲突。
3. **协程（Coroutine）：**

   - 协程是一种轻量级的线程，它在用户空间中进行调度，不依赖于操作系统的线程和进程。协程之间可以通过协作式调度进行切换，而不是被操作系统强制切换。协程的切换开销相对较小，适用于高并发的场景。协程通常由开发者显式地调度，例如通过 yield 或 await 操作。

这三者的主要区别可以总结如下：

- 进程是独立的执行环境，拥有独立的资源，进程之间通常通过 IPC 进行通信。
- 线程是进程中的执行单元，多个线程共享同一个进程的资源，需要考虑同步和互斥。
- 协程是一种轻量级的线程，由用户空间进行调度，切换开销相对较小，适用于高并发场景。

## 11.linux常用命令

在 Linux 系统中，有许多强大而实用的命令，以下是一些常用的命令，它们可以帮助你查看系统信息、文件内容、进程状态等：

1. **系统信息：**

   - `uname -a`：显示系统信息，包括内核版本、操作系统等。
   - `hostname`：显示主机名。
   - `uptime`：显示系统的运行时间和平均负载。
2. **文件内容查看：**

   - `cat`：显示整个文件的内容。
   - `head`：显示文件的前几行。
   - `tail`：显示文件的后几行。
   - `less` 或 `more`：逐页查看文件内容。
3. **文本搜索和过滤：**

   - `grep`：在文件中搜索指定模式。
   - `find`：在文件系统中查找文件。
   - `awk`：用于文本处理，特别适合数据抽取和报告生成。
   - `sed`：流编辑器，用于文本替换和转换。
4. **进程管理：**

   - `ps`：显示当前进程状态。
   - `top`：实时显示系统资源使用情况和进程信息。
   - `kill`：发送信号给进程，用于终止或操作进程。
5. **网络相关：**

   - `ifconfig` 或 `ip`：显示网络接口信息。
   - `ping`：测试网络连接。
   - `netstat`：显示网络状态和连接信息。
   - `traceroute`：跟踪数据包的路径。
6. **用户和权限：**

   - `who`：显示当前登录用户。
   - `w`：显示当前登录用户及其活动。
   - `id`：显示用户的 UID 和 GID。
   - `chmod`：修改文件权限。
7. **系统日志：**

   - `dmesg`：显示内核日志。
   - `journalctl`：显示系统日志。

这只是一小部分常用的 Linux 命令，Linux 系统提供了丰富的命令行工具，可以满足不同的系统管理和开发需求。可以使用 `man` 命令查看命令的手册页，例如 `man ls`。

## 12.Redis批量查询

在 Redis 中，有多种命令可以用于批量查询数据，其中最常见的是 `MGET` 命令。以下是一些批量查询数据的命令：

1. **MGET 命令：**

   - `MGET` 命令用于获取多个键的值。通过指定多个键，可以一次性获取它们的值。

   ```bash
   MGET key1 key2 key3
   ```
2. **Pipeline 批量查询：**

   - 使用 Redis Pipeline 可以在一次请求中发送多个命令，并一次性获取它们的响应。这对于批量查询来说是一种高效的方式。

   ```bash
   MULTI
   GET key1
   GET key2
   GET key3
   EXEC
   ```

   这个示例中，`MULTI` 开启了一个事务，然后在事务中执行了三个 `GET` 命令，最后通过 `EXEC` 提交事务并获取结果。

## 13.浏览器和服务器连接，有哪些步骤

当浏览器与服务器建立连接时，通常经过以下步骤：

1. **DNS 解析：**

   - 当用户在浏览器中输入一个网址时，首先进行 DNS 解析，将域名解析为相应的 IP 地址。这一步骤由本地 DNS 缓存和 DNS 服务器完成。
2. **建立 TCP 连接：**

   - 通过浏览器向服务器发起 TCP 连接请求。这个过程通常经历三次握手，即客户端发送 SYN（同步）请求，服务器回应 SYN-ACK，最后客户端发送 ACK，建立起双向的 TCP 连接。
3. **发起 HTTP 请求：**

   - 一旦建立了 TCP 连接，浏览器就会发送一个 HTTP 请求给服务器。请求中包含用户所需的资源路径、请求方法（GET、POST 等）、头部信息等。
4. **服务器处理请求：**

   - 服务器接收到 HTTP 请求后，会根据请求的内容和服务器上的相应处理逻辑，生成并返回 HTTP 响应。
5. **服务器响应：**

   - 服务器向浏览器发送 HTTP 响应，响应中包含状态码、响应头（Content-Type、Content-Length 等）以及实际的响应体，可能是 HTML、CSS、JavaScript 文件等。
6. **浏览器解析响应：**

   - 浏览器接收到响应后，开始解析响应内容。对于 HTML 内容，浏览器会构建 DOM 树；对于 CSS 文件，浏览器会构建样式表；对于 JavaScript，浏览器会执行脚本。
7. **显示页面：**

   - 最后，浏览器根据解析后的内容渲染页面，呈现给用户。这包括将 HTML 内容显示在屏幕上，应用样式表，执行 JavaScript 以使页面交互。
8. **断开连接：**

   - 一旦页面被完全加载，或者用户离开当前页面，浏览器可能会断开与服务器的 TCP 连接。这个过程通常经历四次挥手，即客户端发送 FIN（结束）请求，服务器回应 ACK，然后服务器发送 FIN，客户端回应 ACK，完成断开连接。

这些步骤构成了浏览器与服务器之间的基本通信过程，被称为 HTTP 请求-响应模型。在这个过程中，HTTP 协议负责定义客户端和服务器之间的通信规则。

## 14.mysql主从同步的原理

MySQL 主从同步（Master-Slave Replication）是一种数据复制机制，它允许将一个 MySQL 主数据库的更改同步到一个或多个从数据库。主从同步的原理可以简要概括为以下几个步骤：

1. **二进制日志（Binary Log）的启用：**

   - 在主数据库上启用二进制日志，主要通过配置文件中的参数 `log-bin` 来实现。二进制日志记录了主数据库上的所有更改操作，包括插入、更新和删除。
2. **主数据库的更改事件记录：**

   - 当在主数据库上执行一条更改操作时（例如，插入一条记录），该操作会被记录到二进制日志中。每个更改都被赋予一个唯一的事件编号。
3. **从数据库连接到主数据库：**

   - 从数据库通过配置文件中的参数（如 `master-host`、`master-port`、`master-user`、`master-password`）配置连接到主数据库。
4. **从数据库获取主数据库的数据快照：**

   - 从数据库通过执行 `SHOW MASTER STATUS` 获取主数据库当前的二进制日志文件名和位置（position）。然后，从数据库通过 `CHANGE MASTER TO` 命令设置自己的复制位置，以获取主数据库的数据快照。
5. **从数据库开始复制：**

   - 从数据库执行 `START SLAVE` 命令，开始从主数据库复制数据。从此刻起，从数据库会不断地连接到主数据库，获取并应用主数据库的二进制日志中的更改。
6. **主从同步的实时复制：**

   - 一旦从数据库完成数据快照的复制，它会保持与主数据库的实时同步。主数据库上的每个更改都会被记录到二进制日志中，并通过复制线程传递给从数据库。
7. **监控主从同步状态：**

   - 可以通过执行 `SHOW SLAVE STATUS` 查看从数据库的同步状态。该命令提供了一系列的参数和状态信息，包括复制的位置、延迟等。
8. **故障处理和恢复：**

   - 如果发生主从同步的中断，可以通过重新配置从数据库的连接参数，获取主数据库的最新数据快照，然后继续复制。

主从同步的机制允许从数据库可以在主数据库上执行只读查询，从而分担主数据库的负载。这也提供了一种数据备份和故障恢复的机制。需要注意的是，主从同步并不保证主从数据库的完全一致性，因为在复制的过程中可能会出现网络延迟或其他因素。

## 15. MySql 内置加密函数

1. **AES 加密和解密：**

   - MySQL 提供了 `AES_ENCRYPT()` 和 `AES_DECRYPT()` 函数，用于进行基于AES的对称加密和解密。

   ```sql
   -- 使用AES加密
   SELECT AES_ENCRYPT('Hello, MySQL', 'secret_key');

   -- 使用AES解密
   SELECT AES_DECRYPT('encrypted_data', 'secret_key');
   ```
2. **MD5 和 SHA 加密：**

   - MySQL 提供了 `MD5()` 和 `SHA1()` 等函数，用于计算给定字符串的 MD5 或 SHA 散列值。

   ```sql
   SELECT MD5('password');
   SELECT SHA1('password');
   ```

   注意：MD5 和 SHA1 是单向散列函数，不可逆。

## 16.systemd 启用流程

1. **创建服务配置文件：**

* 在 `/etc/systemd/system/` 目录中创建以 `.service` 结尾的服务配置文件，例如 `my-service.service`。

2. 边界相关配置信息
3. 加载并启用

常用命令

```shell
# 启动一个服务
systemctl start serviceName

# 停止一个服务
systemctl stop serviceName

# 重启一个服务
systemctl restart serviceName

# 查看服务状态
systemctl status serviceName

# 启用服务（开机自启动）
systemctl enable serviceName

# 禁用服务（开机不自启动）
systemctl disable serviceName

# 查看系统日志
journalctl

# 查看指定服务的日志
journalctl -u serviceName

```

# MySql 汇总

## 1.索引的数据结构 B+树

B+树是一种多叉树，一棵 m 阶的 B+ 树定义如下：

1. 每个节点最多有 m 个子女。
2. 除根节点外，每个节点至少有 [m/2] 个子女，根节点至少有两个子女。
3. 有 k 个子女的节点必有 k 个关键字。

**叶子存放了数据，而非叶子节点只是存放了关键字。**

**叶子节点被链表串联起来了。**

优势：

1. B+ 树的高度和二叉树之类的比起来更低，树的高度代表了查询的耗时，所以查询性能更好。
2. B+ 树的叶子节点都被串联起来了，适合范围查询。
3. B+ 树的非叶子节点没有存放数据，所以适合放入内存中。

## 2.索引分类

• 聚簇索引和非聚簇索引：核心是叶子节点放的是数据本身，还是只是放了一个主键

• 联合索引（组合索引）和非联合索引：使用了多个列的就是联合索引

• 唯一索引和非唯一索引：

• 前缀索引：使用了列的一部分的索引。比如说在varchar(128) 的字段上，值利用前32个字符创建索引；

• 全文索引

• 覆盖索引：其实是指你查询的列，都是某个索引的列。覆盖索引最大的好处就是不用回表

## 3. 索引的代价

索引的代价索引并不是没有代价的，它会消耗很多的系统资源。

1. 索引本身需要存储起来，消耗磁盘空间。
2. 在运行的时候，索引会被加载到内存里面，消耗内存空间。
3. 在增删改的时候，数据库还需要同步维护索引，引入额外的消耗。

## 4.MySQL 为什么使用 B+ 树？

回答这个问题，你就不能仅仅局限在 B+ 树和 B 树上，你要连带着二叉树、红黑树、跳表一起讨论。总结起来，在用作索引的时候，其他数据结构都有一些难以容忍的缺陷。

1. 与 B+ 树相比，平衡二叉树、红黑树在同等数据量下，高度更高，性能更差，而且它们会频繁执行再平衡过程，来保证树形结构平衡。
2. 与 B+ 树相比，跳表在极端情况下会退化为链表，平衡性差，而数据库查询需要一个可预期的查询时间，并且跳表需要更多的内存。
3. 与 B+ 树相比，B 树的数据存储在全部节点中，对范围查询不友好。非叶子节点存储了数据，导致内存中难以放下全部非叶子节点。如果内存放不下非叶子节点，那么就意味着查询非叶子节点的时候都需要磁盘 IO。

## 5.为什么会不使用索引

1. 使用了 !=、LIKE 之类的查询。
2. 字段区分度不大。比如说你的 status 列只有 0 和 1 两个值，那么数据库也有可能不用。
3. 使用了特殊表达式，包括数学运算和函数调用。
4. 数据量太小，或者 MySQL 觉得全表扫描反而更快的时候。

## 6.EXPLAIN 关键字段

1. type：指的是查询到所需行的方式，从好到坏依次是 system > const > eq_ref > ref > range > index > ALL。

* system 和 const 都可以理解为数据库只会返回一行数据，所以查询时间是固定的。
* eq_ref 和 ref 字面意思是根据索引的值来查找。
* range：索引范围扫描。
* index：索引全表扫描，也就是扫描整棵索引。
* ALL：全表扫描，发起磁盘 IO 的那种全表扫描。

1. possible_keys：候选的索引。
2. key：实际使用的索引。
3. rows：扫描的行数。数据库可能扫描了很多行之后才找到你需要的数据。
4. filtered：查找到你所需的数据占 rows 的比例。

## 7.SQL优化

* (1）避免返回不需要的列，尽量使用覆盖索引避免回表
* (2)避免For循环里面查单条数据，改为一条sql查集合。
* (3)建表的时候考虑增加冗余字段，尽可能保持单表查询，而非多表Join.
* (4)在所有的排序场景中，都应该尽量利用索引来排序.
* (5)算count行数的时候，如果业务场景要求不高，可以有一个偏门方法，就是执行explain select * from t where xxxx，在执行计划里面会预估出来大致的行数。
* (6)where 替代 having

## 8.临键锁、间隙锁、行锁

在 MySQL 的 InnoDB 引擎里面，锁是借助索引来实现的。或者说，加锁锁住的其实是索引项，更加具体地来说，**就是锁住了叶子节点。**

1. **间隙锁（Gap Lock）：**

   - 间隙锁是锁定两个键之间的间隙，阻止其他事务在这个间隙内插入新的记录，从而解决幻读问题。但间隙锁并没有锁住具体的记录，而只是锁住了两个键之间的空隙。
   - 间隙锁是在**可重复读**隔离级别下才会生效的
2. **行锁（Row Lock）：**

   - 行锁是直接锁定某一行的记录，阻止其他事务对该行的修改。行锁是一种更精确的锁定方式，但在某些情况下可能导致并发性能下降。
3. **临键锁（Next-Key Lock）：**

   - 临键锁是 MySQL 在可重复读隔离级别下使用的一种锁，它实际上是间隙锁和行锁的结合。它锁定了一个范围，并且包括了具体的记录，以解决幻读问题。临键锁确保事务在读取数据时不受其他事务插入数据的干扰。
   - 它可以解决在可重复读隔离级别之下的幻读问题。

## 9. 加锁的规则

### **两个“原则”、两个“优化”和一个“bug”。**

1、原则 1：加锁的基本单位是 临键锁（next-key lock）。next-key lock 是前开后闭区间。

2、原则 2：查找过程中访问到的对象才会加锁。

3、优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。

4、优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。

5、一个 bug：**唯一索引**上的范围查询会访问到不满足条件的第一个值为止。（此bug在mysql 8.0.18 已修复）

## 10、锁的特性

在 InnoDB 引擎里面，锁是依赖于索引来实现的。

或者说，锁都是加在索引项上的。因此，如果一个查询用了索引，那么会用行锁，**如果没用到任何索引，那么就会用表锁**。

此外，在 MySQL 里面，间隙锁和临键锁是只工作在**可重复读**这个隔离级别下的。

## 11.临键锁引发的死锁

### 问题：

假设说现在数据库中 ID 最大的值是 78。那么如果两个业务进来，同时执行这个逻辑。一个准备插入 id=79 的数据，一个准备插入 id = 80 的数据。如果它们的执行时序如下图，那么你就会得到一个死锁错误。

在线程 1 执行 SELECT FOR UPDATE 的时候，因为 id=79 的数据不存在，所以实际上数据库会产生一个 (78，supremum] 的临键锁。

类似地，线程 2 也会产生一个 (78，supremum] 临键锁。

当线程 1 想要执行插入的时候，它想要获得 id = 79 的行锁。当线程 2 想要执行插入的时候，它想要获得 id = 80 的行锁，这个时候就会出现死锁。因为线程 1 和线程 2 同时还在等着对方释放掉持有的间隙锁。

### 解决：

方案1：使用行锁，从而规避了死锁问题

先插入一个默认的数据。如果没有数据，那么会插入成功；如果有数据，那么会出现主键冲突或者唯一索引冲突，插入失败。那么在插入成功的时候，执行以前数据不存在的逻辑，但是因为此时数据库中有数据，所以不会使用间隙锁。

方案2：放弃悲观锁，使用乐观锁。

```mysql
UPDATE xxx SET data = newData WHERE id = 1 AND data = oldData。
```

## 12.脏读，不可重复读，幻读

* 脏读是指读到了别的事务还没有提交的数据。之所以叫做“脏”读，就是因为未提交数据可能会被回滚掉。
* 不可重复读是指在一个事务执行过程中，对同一行数据读到的结果不同。
* 幻读是指在事务执行过程中，别的事务插入了新的数据并且提交了，然后事务在后续步骤中读到了这个新的数据。

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是**间隙锁 (Gap Lock)**。在语句执行中，在一行行扫描的过程中，**不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁**。

## 13. 为什么有了锁，还需要 MVCC

避免读写阻塞

1. **锁的机制：**
   * **悲观锁：** 基于锁的并发控制机制是一种悲观的机制，它认为并发访问会导致冲突。在事务执行期间，它会使用锁来限制其他事务对相同资源的访问，以确保事务的一致性。
   * **问题：** 悲观锁可能导致大量的阻塞和等待，尤其是在高并发环境下，因为多个事务可能需要等待对相同资源的独占访问。
2. **MVCC 的机制：**
   * **乐观锁：** MVCC 是一种乐观的并发控制机制，它假设并发访问不会导致冲突。每个事务在开始时读取数据的版本号，并在提交时比较版本号，以确定是否有其他事务对数据进行了更改。
   * **优势：** MVCC 可以提高并发性能，因为事务可以并行执行而无需互相阻塞。

## 14 MVCC

MVCC 是 MySQL InnoDB 引擎用于控制数据并发访问的协议。

MVCC 主要是借助于版本链来实现的。

在 InnoDB 引擎里面，每一行都有两个额外的列，一个是 trx_id，代表的是修改这一行数据的事务 ID。另外一个是 roll_ptr，代表的是回滚指针。

InnoDB 引擎通过回滚指针，将数据的不同版本串联在一起，也就是版本链。这些串联起来的历史版本，被放到了 undolog 里面。当某一个事务发起查询的时候，MVCC 会根据事务的隔离级别来生成不同的 Read View，从而控制事务查询最终得到的结果。

## 15.Read View

Read View 只用于已提交读和可重复读两个隔离级别，它用于这两个隔离级别的不同点就在于什么时候生成 Read View。

* 已提交读：事务每次发起查询的时候，都会重新创建一个新的 Read View。
* 可重复读：事务开始的时候，创建出 Read View。

## 16. redo log 和 bin log

这两种日志有以下三点不同。

1、redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

2、redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

3、redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，**并不会覆盖以前的日志**。

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

## 17.redo log

有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面。

（InnoDB引擎先把记录写到redo log 中，redo log 在哪，他也是在磁盘上，这也是一个写磁盘的过程，但是与更新过程不一样的是，更新过程是在磁盘上随机IO，费时。 而写redo log 是在磁盘上顺序IO。效率要高。）

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

## 18. undo log

MySQL的Undo Log（回滚日志）是一种用于实现事务的关键组件，它记录了事务执行过程中所做的修改，以便在事务回滚或数据库崩溃时进行数据的恢复。Undo Log 是实现数据库事务的ACID特性（原子性、一致性、隔离性、持久性）的重要机制之一。

以下是 Undo Log 的主要特点和作用：

1. **记录事务修改：** 当事务执行更新、插入或删除操作时，MySQL会在Undo Log中记录这些修改的详细信息，包括修改前的数据内容。
2. **用于回滚：** 如果事务需要回滚（Rollback），Undo Log中的信息可以用于撤销事务所做的修改，将数据还原到事务开始之前的状态。
3. **多版本并发控制（MVCC）：** Undo Log 也与多版本并发控制（MVCC）有关。在MVCC中，每个事务在修改数据时会创建一个新版本，而老版本的数据则通过Undo Log进行保留，以便其他事务在读取数据时可以看到一致的快照。
4. **事务的隔离性：** Undo Log 的使用有助于实现数据库事务的隔离性。当一个事务正在修改某个数据时，其他事务可以通过Undo Log来获取该数据的老版本，从而避免读取到未提交的数据。
5. **持久性保证：** Undo Log 的数据会持久保存在磁盘上，以确保在数据库崩溃或异常情况下，可以通过Undo Log进行数据的恢复。

总体来说，Undo Log 是 MySQL 中一项关键的机制，它确保了事务的一致性和隔离性，同时支持了事务的回滚和数据库的恢复。 Undo Log 的设计使得 MySQL 能够有效地处理并发事务和保障数据库的稳定性。

## 19.数据迁移方案

基本步骤。

1. 创建目标表。
2. 用源表的数据初始化目标表。
3. 执行一次校验，并且修复数据，此时用源表数据修复目标表数据。(有update_time字段，根据这个增量修复)
4. 业务代码开启双写，此时读源表，并且先写源表，数据以源表为准。
5. 开启增量校验和数据修复，保持一段时间。
6. 切换双写顺序，此时读目标表，并且先写目标表，数据以目标表为准。
7. 继续保持增量校验和数据修复。
8. 切换为目标表单写，读写都只操作目标表。

### mysqldump数据导入导出优化

1. 加快导出速度能做的事情并不多，主要就是开启 extended-insert 选项，将多行合并为一个 INSERT 语句。

```shell
  mysqldump --extended-insert -u your_username -p your_password your_database > dump.sql
```

2. 加快导入速度就可以做比较多的事情。

* 关闭唯一性检查和外键检查，源表已经保证了这两项，所以目标表并不需要检查。

```mysql
  mysql> SET foreign_key_checks = 0;
  mysql> SET unique_checks = 0;
```

* 关闭 binlog，毕竟导入数据用不着 binlog。

```mysql
  SET sql_log_bin = 0;
```

* 调整 redo log 的刷盘时机，把innodb_flush_log_at_trx_commit 设置为 0。

## 20.索引创建规则

* 在 WHERE 条件里面经常出现的。比如说外键；
* 使用有很多不同值的列：所以类似于 Status 这种枚举的效果就不是很好；
* 不要使用很长的列：比如说 BLOB 这种，或者很长的 varchar。一定要用的话，创建前缀索引；

创建联合索引，确定索引的顺序：

• 选择性高的在前面；

• 经常用作范围查询（也就是会中断索引使用的）放在后面；

# 缓存及Redis

## 1.redis的缓存删除方式

1. **定期删除（定时删除）** ：
   Redis 会按照一定的时间间隔，对设置了过期时间的键进行定期检查，并删除已经过期的键。这个过期检查并不是精确的，而是以一定的概率进行，避免对系统性能产生过大的影响。这个过期删除策略通过配置文件中的 `hz`（每秒执行的周期性操作的次数）参数来控制，默认值为 10。你可以通过修改 Redis 配置文件中的 `hz` 参数来调整过期删除的频率。正常来说，这个值不需要调，即便调整也不要超过 100。与之相关的是 dynamic-hz 参数。这个参数开启之后，Redis 就会在 hz 的基础上动态计算一个值，用来控制后台任务的执行频率。

> 在每一个定期删除循环中，Redis 会遍历 DB。如果这个 DB 完全没有设置了过期时间的 key，那就直接跳过。否则就针对这个 DB 抽一批 key，如果 key 已经过期，就直接删除。
>
> 如果在这一批 key 里面，过期的比例太低，那么就会中断循环，遍历下一个 DB。如果执行时间超过了阈值，也会中断。不过这个中断是整个中断，下一次定期删除的时候会从当前 DB 的下一个继续遍历。
>
> 总的来说，Redis 是通过控制执行定期删除循环时间来控制开销，这样可以在服务正常请求和清理过期 key 之间取得平衡。

2. **惰性删除（懒汉式删除）** ：
   当客户端访问某个键时，Redis 会先检查这个键是否过期，如果过期则会删除。这个过程称为惰性删除，因为删除操作是在键被访问时进行的。

### 为什么不立刻删除

理论上来说，并不是做不到，只不过代价比较高昂不值得而已。

最简单的做法就是使用定时器，但是定时器本身开销太大，还得考虑在更新过期时间的时候重置定时器。

另外一种思路就是使用延迟队列，但是延迟队列本身开销也很大，修改过期时间也要调整延迟队列，还要引入大量的并发控制。

综合来看，并不值得。而定期删除和懒惰删除的策略虽然看上去可能浪费内存，但是这个浪费很少，并且对响应时间也没那么大的影响。

## 2. Redis 持久化怎么处理过期 key？

对于 RDB 来说，主库不读不写，从库原封不动。

对于 AOF 来说，正常追加 DEL 命令，重写则是不管。

## 3.redis淘汰策略

Redis 中的淘汰策略指的是在内存不足时，用于选择要删除的键的规则。Redis 使用以下几种淘汰策略：

1. **LRU（Least Recently Used，最近最少使用）**：

   - 在 LRU 策略中，系统会根据键的最近使用情况来判断哪些键最久未被使用，然后删除最久未被使用的键。
   - Redis 中提供了两种 LRU 实现方式，分别是近似 LRU 和精确 LRU。近似 LRU 使用了一些概率算法，而精确 LRU 通过维护一个有序链表来实现。
2. **LFU（Least Frequently Used，最不经常使用）**：

   - LFU 策略根据键被访问的频率来判断哪些键使用频率最低，然后删除使用频率最低的键。
   - 在 Redis 中，LFU 策略也有近似和精确两种实现方式。
3. **随机删除**：

   - 随机删除策略是随机选择一个键进行删除，这种方式比较简单，但可能导致一些热点数据被删除。
4. **先进先出（FIFO）**：

   - FIFO 策略按照键被插入的顺序来删除键，最先插入的键最先被删除。

淘汰策略的选择可以通过 Redis 配置文件中的 `maxmemory-policy` 参数来设置。例如：

```bash
maxmemory-policy volatile-lru
```

上述配置表示使用 LRU 策略，且只对设置了过期时间的键进行淘汰。其他可选的值还包括 `allkeys-lru`、`volatile-lfu`、`allkeys-lfu`、`volatile-random`、`allkeys-random`、`volatile-ttl` 和 `noeviction`。

请注意，淘汰策略只有在 Redis 达到配置的内存限制时才会生效。

## 4. redis的常用命令

以下是 Redis 中一些常用的命令：

1. **字符串操作**：

   - `SET key value`：设置键的字符串值。
   - `GET key`：获取键的字符串值。
   - `DEL key`：删除键。
2. **哈希表操作**：

   - `HSET key field value`：在哈希表中设置字段的值。
   - `HGET key field`：获取哈希表中字段的值。
   - `HDEL key field1 field2`：删除哈希表中一个或多个字段。
   - `HMSET key field1 value1 [field2 value2 ...]` 批量设置字段
   - `HGETALL key` 获取所有的字段和值
3. **列表操作**：

   - `LPUSH key value1 value2`：在列表左侧插入一个或多个值。
   - `RPUSH key value1 value2`：在列表右侧插入一个或多个值。
   - `LPOP key`：从列表左侧弹出一个值。
   - `RPOP key`：从列表右侧弹出一个值。
   - `LRANGE key start stop` 获取指定范围元素
4. **集合操作**：

   - `SADD key member1 member2`：向集合添加一个或多个成员。
   - `SMEMBERS key`：获取集合的所有成员。
   - `SREM key member1 member2`：从集合中移除一个或多个成员。
5. **有序集合操作**：

   - `ZADD key score1 member1 score2 member2`：向有序集合添加一个或多个成员，同时指定分数。
   - `ZRANGE key start stop`：按照分数范围获取有序集合的成员。
   - `ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]` 按照分数范围获取成员
   - `ZREMRANGEBYRANK key start stop` 移除有序集合中给定排名范围内的所有成员。
   - `ZREMRANGEBYSCORE key min max` 移除有序集合中给定分数范围内的所有成员。
6. **键操作**：

   - `EXPIRE key seconds`：设置键的过期时间。
   - `TTL key`：获取键的剩余过期时间。
   - `DEL key`：删除键。
7. **其他常用命令**：

   - `PING`：检查 Redis 服务器是否运行。
   - `INFO`：获取 Redis 服务器的信息。
   - `FLUSHDB`：清空当前数据库中的所有键。
   - `FLUSHALL`：清空所有数据库中的所有键。

这只是 Redis 命令的一小部分，Redis 支持丰富的数据结构和操作命令，具体的使用可以根据需求选择相应的命令。

## 5. 缓存模式

Cache Aside、Read Through、Write Through、Write Back、Singleflight。

1. **Cache Aside (Cache-Through)**：

   - 在这种模式下，应用程序负责管理缓存。读取时，应用程序首先尝试从缓存中获取数据，如果不存在，则从持久存储中读取，并将数据放入缓存。写入时，应用程序负责更新缓存和写入持久存储。
   - 这是一种简单的模式，适用于许多场景，但需要应用程序处理缓存一致性。
   - 在这个模式下，业务代码就是把缓存看成是和数据库一样的独立的数据源，然后业务代码控制怎么写入缓存，怎么写入数据库。一般来说，都是优先写入数据库的。
2. **Read Through**：

   - 在这种模式下，应用程序无需直接管理缓存。读取时，应用程序从缓存请求数据，如果缓存中不存在，则缓存负责从持久存储中读取数据并更新缓存。
   - 这减轻了应用程序对缓存的管理负担，但可能引入缓存一致性的问题。
   - 异步方案变种：1、缓存可以在从数据库加载了数据之后，立刻把数据返回给业务代码，**然后开启一个线程异步更新缓存**。2、第二个变种是直接让整个加载和回写缓存的过程都异步执行。也就是说，如果缓存未命中，那么就直接返回一个错误或者默认值，然后缓存异步地去数据库中加载，并且回写缓存。和第一个变种比起来，这种变种的缺陷是业务方在当次调用中只能拿到错误或者默认值。
3. **Write Through**：

   - Write Through 就是在写入数据的时候，只写入缓存，然后缓存会代替我们的去更新数据库。但是，Write Through 没有要求先写数据库还是先写缓存，不过一般也是先写数据库。
   - 其次，Write Through 也没有讨论如果缓存中原本没有数据，那么写入数据的时候，要不要更新缓存。一般来说，如果预计写入的数据很快就会读到，那么就要刷新缓存中的数据。
   - 异步变种：1、在缓存收到写请求之后，可以直接返回成功响应，然后异步写入数据库和刷新缓存。但是这种方案比较危险，存在数据丢失的风险。 2、缓存也可以考虑只写入数据库，然后返回成功响应，后面可以异步刷新缓存。基本上前者很少用，要用也是用一个和它很像的 Write Back 方案。变种二则适合用于缓存写入操作且代价高昂的场景。比如说前面提到的，写入大对象或者需要序列化大对象再写入缓存。
4. **Write Back**：

   - Write Back 模式是指我们在更新数据的时候，只把数据更新到缓存中就返回。后续会有一个组件监听缓存中过期的 key，在过期的时候将数据刷新到数据库中。
   - 显然，只是监听过期 key 的话还是会有问题，比如说关闭缓存的时候还是需要把缓存中的数据全部刷新到数据库里。
   - Write Back 最大的优点是排除数据丢失这一点，它能解决数据一致性的问题。

   > 首先，在使用 Redis 更新数据的时候业务代码只更新缓存，所以对于业务方来说必然是一致的。也就是说，虽然数据库的数据和缓存的数据不一致，但是对于业务方来说，它只能读写到缓存的数据，对业务方来说，数据是一致的。
   > 当业务方读数据的时候，如果缓存没有数据，就要去数据库里面加载。这个时候，就有可能产生不一致的问题。比如说，数据库中 a=3，读出来之后还没写到缓存里面。这个时候来了一个写请求，在缓存中写入了 a = 4。如果这时候读请求回写缓存，就会用数据库里的老数据覆盖缓存中的新数据。
   > 解决这个问题的思路也很简单，当读请求回写的时候，使用 SETNX 命令。也就是说，只有当缓存中没有数据的时候，才会回写数据。而如果回写失败了，那么读请求会再从缓存中读取到数据。
   >
5. **Singleflight**：

   - 这是一种通过调度系统中相同的请求，以确保只有一个请求被实际执行的缓存模式。这可以用于减轻“缓存击穿”问题，其中大量请求同时访问缓存中不存在的数据。
6. 删除缓存

   - 删除是最常用的更新缓存的模式，它是指在更新数据库之后，直接删除缓存。
   - 这种做法可以是业务代码来控制删除操作，也可以结合 Write Through 使用。而且删除缓存之后会使缓存命中率下降，也算是一个隐患。
   - 如果偶尔出现写频繁的场景，导致缓存一直被删除，那么就会使性能显著下降。缓存未命中回查数据库叠加写操作，数据库压力会很大。
   - 删除缓存和别的模式一样，也有一致性问题。但是它的一致性问题是出在读线程缓存未命中和写线程冲突的情况下。

## 6.什么是缓存穿透、雪崩、击穿？如何解决？

### 缓存穿透

缓存穿透是指数据既不在缓存中，也不在数据库中。

解决：

1、第一种思路是回写特殊值。

也就是在第一次查询发现数据库里都没有数据的时候，直接写入一个特殊值。那么下一次查询过来的时候，看到缓存里的特殊值，就知道没有数据，这时候直接返回就可以了。在这种设计之下，数据库只需要支撑住第一次请求就可以。

2、布隆过滤器

### 缓存击穿

缓存击穿是指数据不在缓存中，导致请求落到了数据库上。(大量热点数据)

解决：singleflight 模式

### 缓存雪崩

缓存雪崩是指缓存里大量数据在同一时刻过期，导致请求都落到了数据库上。

解决：

要解决缓存雪崩，就是在数据批量加载到缓存的场景中在过期时间的基础上加上一个随机量。

比如说，我预计这一批数据的过期时间是 15 分钟。那么我就在设置每一条数据的过期时间的时候，在 15 分钟的基础上加上一个 0～180 秒的偏移量。那么这一批数据就不会在同一时刻过期，也就不存在缓存雪崩的问题了。

偏移量要跟过期时间成正比，不能过低或者过高。

## 7. 分布式锁

## 8. 数据一致性

要想彻底解决数据一致性问题，就首先要搞清楚数据不一致的两个来源。

第一个是操作部分失败，第二个是并发更新。

源自部分失败造成的数据不一致是不可避免的。（现在缓存中间件包括 Redis，都不支持分布式事务。因此这个问题就决定了数据不一致是不可避免的。）

要想解决并发操作带来的问题，可以使用缓存模式、分布式锁、消息队列或者版本号。

#### 消息队列

针对一些可以异步更新数据的场景，可以考虑将更新请求都发送到消息队列上。

但是要注意的是，同一个业务的消息必须是有序的，不然更新数据会出错。消费者在取出消息之后，执行更新。而且消费者在更新失败之后，可以多次重试，对业务也没有什么影响。这个方案也是追求最终一致性的，强一致性还是用不了。

#### 版本号

每个数据都有一个对应的版本号，在更新的时候版本号都要加一。每一次在更新的时候都要比较版本号，版本号低的数据不能覆盖版本号高的数据。

这个方案的缺点是需要维护版本号，最好是在数据库里面增加一个版本字段。那么后面在更新缓存的时候，比如说更新 Redis，就得使用 lua 脚本，先检测缓存的版本号，再执行更新了。不过也可以考虑用更新时间来替代版本号，一样可以。

#### 分布式锁

**先本地事务，后分布式锁**

先执行本地事务，然后加分布式锁，删除缓存，释放分布式锁。

我之前用过分布式锁来尝试解决并发更新的问题，

基本思路是先执行本地事务，在事务提交之后加分布式锁，然后删除缓存。

而在读请求来了的时候，先读缓存。如果缓存未命中，再加分布式锁，然后从数据中加载数据回写缓存，再释放分布式锁

## 9. 分布式锁知识点

利用 Redis 来实现分布式锁的时候，**所谓的锁就是一个普通的键值对**。而加锁就是使用 SETNX 命令，排他地设置一个键值对。如果 SETNX 命令设置键值对成功了，那么说明加锁成功。如果没有设置成功，说明这个时候有人持有了锁，需要等待别人释放锁。而相应地，释放锁就是删除这个键值对。

使用 Lua 脚本尝试获取锁，这边是有三种情况：
// 1.key 不存在
// 2.你上次加锁成功了但是返回超时了
// 3.锁被人家拿着

主要组成部分：

1. **Client 结构体**：封装了 Redis 客户端和 singleflight 机制。`SingleflightLock` 方法通过 `singleflight` 包装了 `Lock` 方法，确保对相同 key 的并发请求只会执行一次实际的获取锁操作。
2. **Lock 结构体**：代表一个锁实例，包含锁的键、值、过期时间等信息。提供了 `AutoRefresh` 方法用于自动续约锁的过期时间，并在解锁时通知自动续约的 goroutine 退出。
3. **Lock 方法**：`Lock` 方法用于尝试获取锁，内部使用 Lua 脚本通过 `SETNX` 命令来实现原子性的锁获取。在获取锁失败时，通过重试策略（`RetryStrategy` 接口）来决定是否继续重试。
4. **AutoRefresh 方法**：用于自动续约锁的过期时间，通过定时器周期性地执行刷新操作。在刷新过程中，如果遇到超时或其他错误，会通过通道通知下一次刷新。
5. **Refresh 方法**：手动刷新锁的过期时间，使用 Lua 脚本通过 `EVAL` 命令来实现原子性的刷新操作。
6. **TryLock 方法**：尝试获取锁，使用 `SETNX` 命令实现，如果锁已被其他协程持有，则返回获取失败。
7. **Unlock 方法**：释放锁，通过 Lua 脚本实现原子性的解锁操作。

总体来说，这段代码实现了一个基于 Redis 的分布式锁，考虑了并发情况下的重复请求和自动续约机制。


# Go相关

1. gorm默认更新（updates）默认值，如：0，false，null等不会更
