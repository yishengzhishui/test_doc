---
typora-copy-images-to: upload
---
## 基础篇

### 1、查询语句的执行

#### MySQL的基本架构示意图

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。

Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

#### 连接器

第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。

连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：

```
mysql -hip -Pport -u$user -p// mysql -u root -p
```

连接命令中的 mysql 是**客户端工具**，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。(mysqld 是服务端，mysql 是客户端)

数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以我建议你在使用中要**尽量减少建立连接的动作，也就是尽量使用长连接**。

##### 长连接

全部使用长连接后， MySQL 占用内存可能涨得特别快。这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能**导致内存占用太大**，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。

你可以考虑以下两种方案。1、定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。2、如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

#### 查询缓存

MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。

但是大多数情况下我会建议你**不要使用查询缓存**，为什么呢？因为查询缓存往往弊大于利。

1。缓存需要语句完全相等，包括参数。2。表更新后就会失效 因此，只有在表更新频率不高，查询语句完全一致的情况下。

好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 **SQL_CACHE 显式指定**，像下面这个语句一样：

```mysql
select SQL_CACHE * from T where ID=10
```

注意：mysql8之后，取消了缓存功能。

#### 分析器

如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。

#### 优化器

经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

优化器是在表里面有多个**索引**的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。

例：

```mysql
mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;
```

既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。

也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

#### 执行器

进入了执行器阶段，开始执行语句。

##### 权限验证

开始执行的时候，要先判断一下你对这个表 T 有没有**执行查询的权限**，如果没有，就会返回没有权限的错误，(在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在**优化器之前**调用 precheck 验证权限)。

权限验证不仅仅在执行器这部分会做，在**分析器之后**，也就是知道了该语句要“干什么”之后，也会先做一次权限验证。叫做**precheck**。而precheck是无法对运行时涉及到的表进行权限验证的，比如使用了触发器的情况。因此在执行器这里也要做一次执行时的权限验证。

mysql的权限验证阶段，在执行器执行环节为什么还要进行验证，因为除了sql还可能有存储引擎，触发器等，在这些对象中，也可能需要调用其它表去获取数据，也需要权限验证，如果在分析节点之前就进行对象的验证，那么对于触发器，存储引擎这种对象的执行是做不到的。

### 2、更新语句的执行

DDL(data definition language) 数据库定义语言

DML(data maninpulation language) 数据库操作语言

redo log（重做日志-InonoDB引擎特有） 和 binlog（归档日志-Sever层）

WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。（“先写日志” 也是先写磁盘，只是写日志是顺序IO，速度很快）

具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面。

（InnoDB引擎先把记录写到redo log 中，redo log 在哪，他也是在磁盘上，这也是一个写磁盘的过程，但是与更新过程不一样的是，更新过程是在磁盘上随机IO，费时。 而写redo log 是在磁盘上顺序IO。效率要高。）

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

这两种日志有以下三点不同。

1、redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

2、redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

3、redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，**并不会覆盖以前的日志**。

#### 执行流程

执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。

1、执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 **这一行所在的数据页**本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。（所以对于更新操作，并不会更新某条记录就把某条记录查询到内存中对其做修改就行，而是将**对应记录所在页**都加载到内存中。）

2、执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3、引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

4、执行器生成这个操作的 binlog，并把 binlog 写入磁盘。

5、执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。

![2e5bff4910ec189fe1ee6e2ecc7b4bbe](https://static001.geekbang.org/resource/image/2e/be/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

#### 两阶段提交

将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

1、首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；

2、然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。

### 3、事物隔离

简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。

#### 脏读、不可重复读、幻读的区别是什么？

脏读：一个事务读取了其它事务update后未提交的数据，注意是针对其它事务的**修改未提交**的操作。

不可重复读：一个事务读取了其它事务update或delete后**已提交的数据**，注意是针对其它事务修改或删除已提交的操作。

幻读：一个事务读取了其它事务新insert已经提交的数据。注意是针对其它事务**新增已提交**的数据。

#### 隔离级别

你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。

SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。

读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。

读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。

可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。

串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。

若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。

若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。

若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，**会被锁住**。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。

在实现上，数据库里面会创建一个**视图**，访问的时候以视图的逻辑结果为准。

在“可重复读”隔离级别下，这个视图是在**事务启动时**创建的，整个事务存在期间都用这个视图。

在“读提交”隔离级别下，这个视图是在每个 SQL **语句开始执行**的时候创建的。（所以一个事务是可以看到另外一个事务已经提交的内容，因为它在每一次查询之前都会**重新给予最新的数据**创建一个新的MVCC视图。）

这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

#### 事务隔离的实现

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。（同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC））

回滚日志总不能一直保留吧，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。（当没有比回滚日志更早的读视图（读视图在事务开启时创建）的时候，这个数据不会再有谁驱使它回滚了，这个回滚日志也就失去了用武之地，可以删除了）

建议你尽量不要使用长事务。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

#### 事务的启动方式

MySQL 的事务启动方式有以下几种：

1、显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。

2、set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。

如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

#### 如何避免长事务对业务的影响？

##### 从应用开发端来看：

1、确认是否使用了 set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。

2、确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。

业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令（该参数是用来控制select语句的最大执行时间,单位毫秒;），来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。

#### 从数据库端来看：

1、监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；

2、Percona 的 pt-kill 这个工具不错，推荐使用；

3、在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；

4、如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。

### 4、索引

简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。

#### InnoDB 的索引模型

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。（B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。）

每一个索引在 InnoDB 里面对应一棵 B+ 树。

索引类型分为**主键索引**和**非主键索引**。

主键索引的叶子节点存的是**整行数据**。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。

非主键索引的叶子节点内容是**主键的值**。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

基于主键索引和普通索引的查询有什么区别？

如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；

如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为**回表**。

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

#### 索引维护

主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

所以，从性能和存储空间方面考量，**自增主键**往往是更合理的选择。

#### 覆盖索引

如果执行的语句是 `select ID from T where k between 3 and 5`，这时**只需要查 ID 的值**，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为**覆盖索引**。

覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

如果现在有一个高频请求，要**根据市民的身份证号查询他的姓名**，这个联合索引就有意义了,建立一个（身份证号、姓名）的联合索引。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。(这种情况，根据**索引树上的键就是(身份证号，姓名）**，按着身份证号查询的时候，索引节点也会保存姓名，这样就覆盖到了。)

#### 最左前缀原则

B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。

只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是**联合索引的最左 N 个字段**，也可以是**字符串索引的最左 M 个字符**。

##### 在建立联合索引的时候，如何安排索引内的字段顺序。

第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

第二考虑的原则就是空间了。比如这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。（因为查询条件只有age的是不能使用联合索引的，因此维护两个索引就需要考虑空间）

#### 索引下推

可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

#### 问题-主键索引也是可以使用多个字段

有这么一个表，表结构定义类似这样的：

```mysql
CREATE TABLE `geek` (
  `a` int(11) NOT NULL,
  `b` int(11) NOT NULL,
  `c` int(11) NOT NULL,
  `d` int(11) NOT NULL,
  PRIMARY KEY (`a`,`b`),
  KEY `c` (`c`),
  KEY `ca` (`c`,`a`),
  KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;
```

由于历史原因，这个表需要 a、b 做联合主键

```mysql
//业务code
select * from geek where c=N order by a limit 1;
select * from geek where c=N order by b limit 1;
```

为什么要创建“ca”“cb”这两个索引？

主键 a，b 的聚簇索引组织顺序相当于 order by a,b ，也就是先按 a 排序，再按 b 排序，c 无序。

对于二级索引C，会默认和主键做联合索引。所以**索引c的排序为cab**，索引**cb的排序顺序为cba**。（InnoDB会把主键字段放到索引定义字段后面， 当然**同时也会去重**。）

所以，结论是 ca 可以去掉，cb 需要保留。

#### 问题-重建索引

为什么重建索引？

索引可能因为删除，或者页分裂等原因，导致**数据页有空洞**（删除数据并没有真正的释放空间），重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。

如果你要重建索引 k，你的两个 SQL 语句可以这么写：

```mysql
alter table T drop index k;
alter table T add index(k);
```

如果你要重建主键索引，也可以这么写：

```mysql
alter table T drop primary key;
alter table T add primary key(id);
```

重建索引 k 的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。**不论是删除主键还是创建主键，都会将整个表重建。**因此这两个语句，你可以用这个语句代替 ： `alter table T engine=InnoDB`

### 5、全局锁、表锁和行锁

#### 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 `Flush tables with read lock (FTWRL)`。当你需要让**整个库处于只读**状态的时候，可以使用这个命令，之后其他线程的以下语句会**被阻塞**：

数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

全局锁的典型使用场景是，做**全库逻辑备份**。也就是把整库每个表都 select 出来存成文本。

**但是让整库都只读**，听上去就很危险：

如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；

如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数`–single-transaction `的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。

#### 表级锁

MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。

需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。(对表加**读锁**后，自己也不能对其进行修改；自己和其他线程**只能读**取该表。 当对某个表执加上**写锁**后（lock table t2 write），**该线程可以对这个表进行读写**，**其他线程**对该表的读和写都受到**阻塞**；)

对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。

另一类表级的锁是 MDL（metadata lock)。MDL **不需要显式**使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。

因此，在 MySQL 5.5 版本中引入了 MDL，

当对一个表做**增删改查**操作的时候，加 MDL **读锁**；

当要对表做**结构变更**操作的时候，加 MDL **写锁**。

**读锁之间不互斥**，因此你可以有多个线程同时对一张表增删改查。

**读写**锁之间、**写锁之间是互斥**的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

##### 如何安全地给小表加字段？

1、首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。

2、如果变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？这时候 kill 可能未必管用，因为新的请求马上就来了。**比较理想的机制是**：在 alter table 语句里面**设定等待时间**，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，**拿不到也不要阻塞后面的业务语句**，先放弃。之后开发人员或者 DBA 再**通过重试命令重复**这个过程。

> **表锁**一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：
>
> 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
>
> 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，**最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit**，问题就解决了。

#### 行锁

顾名思义，**行锁就是针对数据表中行记录的锁**。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

##### 两阶段锁

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要**等到事务结束**时才释放。这个就是**两阶段锁**协议。

提高并发度：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

##### 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为**死锁**。

当出现死锁以后，有两种策略：

1、一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。但是一般不使用这种方法

2、另一种策略是，发起**死锁检测**，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。（死锁检测的复杂度是o(n^2)）

例如：并发更新同一行的1000个线程，整体耗费的死锁检测为100万量级。 并发更新此行R1的某单个线程Tx，其所作的死锁检测工作为，Tx会有查看锁持有情况，耗费1000此操作——**a.查看自身持有的行锁; b.遍历其他999个线程所持有的行锁，总共为1+999=1000次**。 为什么会遍历其他999个线程，而不是仅看当前持有R1行锁的这个线程就行了？—— 因为行锁排队。

##### 怎么解决由这种热点行更新导致的性能问题呢

**控制并发度**：这个并发控制要做在数据库服务端，对于相同行的更新，在进入引擎之前排队。

**一行改成逻辑上的多行来减少锁冲突**。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

##### 问题1

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

第一种，直接执行 delete from T limit 10000;

第二种，在一个连接中循环执行 20 次 delete from T limit 500;

第三种，在 20 个连接中同时执行 delete from T limit 500。

方案一，事务相对较长，则占用锁的时间较长，会导致其他客户端等待资源时间较长。
方案二，串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。
方案三，人为自己制造锁竞争，加剧并发量。

##### 问题2

备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？

假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：

```mysql

Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */
```

在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);

启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；

设置一个保存点，这个很重要（Q3）；

show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。

参考答案如下：

1、如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。

2、如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止；

3、如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。

4、从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。

### 6、事务隔离与锁关系（查询与更新有区别）

#### 事务启动的时机

begin/start transaction 命令并不是一个事务的起点，在**执行**到它们之后的**第一个操作 InnoDB 表的语句，事务才真正启动**。如果你想要**马上启动**一个事务，可以使用 `start transaction with consistent snapshot `这个命令。

> 第一种启动方式，一致性视图是在执行第一个快照读语句时创建的；
>
> 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。

#### 视图

在 MySQL 里，有两个“视图”的概念：

一个是 view。它是一个用查询语句定义的**虚拟表**，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。

另一个是 InnoDB 在实现 MVCC 时用到的**一致性读视图**，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。

#### “快照”在 MVCC 里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。（库快照，不是行快照。所以不会进行MDL读锁等待，而是获取所有已提交的数据）

InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按**申请顺序严格递增**的。（先开始的事务获取的事务ID总是小于后开启的事务ID。 **只读事务的ID和非只读事务的ID是有些区别**的。前者是一个很大的数，后者是一个从1自增的数值。）

而**每行数据**也都是有**多个版本**的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 **row trx_id**。同时，**旧的数据版本要保留**，并且在新的数据版本中，能够有信息可以直接拿到它。

**也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。**

（获得不同版本的数据是靠当前数据+undo log(回滚日志)），下图三个虚线（U1,U2,U3）就是undo log，V1，V2，V3并不是物理上真实存在的。

![68d08d277a6f7926a41cc5541d3dfced](http://ipic.jsgsjz.cn/2021-02-04-011125.png)

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。

在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“**活跃”指的就是，启动了但还没提交**。

**数组**里面事务 **ID 的最小值**记为**低水位**（低水位也还是未提交的事务），当前系统里面**已经创建过的事务 ID 的最大值**加 1 记为**高水位**。这个视图**数组**和高水位，就组成了当前事务的**一致性视图**（read-view）。

![882114aaf55861832b4270d44507695e](https://static001.geekbang.org/resource/image/88/5e/882114aaf55861832b4270d44507695e.png)

这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：

1、如果落在绿色部分（小于低水位），表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；

2、如果落在红色部分（高于高水位），表示这个版本是由将来启动的事务生成的，是肯定不可见的；

3、如果落在黄色部分，那就包括两种情况：

a. 若 row trx_id **在数组**中，表示这个版本是由还**没提交**的事务生成的，不可见；

b. 若 row trx_id **不在数组**中，表示这个版本是**已经提交**了的事务生成的，可见。

InnoDB 利用了“**所有数据都有多个版本**”的这个特性，**实现了“秒级创建快照”的能力。**

一个数据版本，对于一个事务**视图**来说，除了自己的更新总是可见以外，有三种情况：

1、版本未提交，不可见；

2、版本已提交，但是是在视图创建后提交的，不可见；

3、版本已提交，而且是在视图创建前提交的，可见。

#### 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**

除了 update 语句外，select 语句如果加锁，也是当前读。（lock in share mode 或 for update）

下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。

```mysql
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

**可重复读的核心就是一致性读**（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

而**读提交的逻辑和可重复读**的逻辑类似，它们最主要的区别是：

在**可重复读**隔离级别下，只需要在**事务开始**的时候创建一致性视图，之后事务里的其他查询都**共用这个一致性视图**；对于可重复读，查询只承认在事务启动前就已经提交完成的数据；

在**读提交**隔离级别下，**每一个语句执行前都会重新算出一个新的视图**。对于读提交，查询只承认在语句启动前就已经提交完成的数据；

## 实践篇

### 1、普通索引和唯一索引的选择

#### 查询过程

普通索引跟唯一索引执行上的区别：

普通索引的等值查询，会继续遍历到第一个不相等的值才会结束，

唯一索引等值查询，命中则结束（**性能差距微乎其微**）

原因是：InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。（在 InnoDB 中，每个数据页的大小默认是 16KB）。当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，**就只需要一次指针寻找和一次计算**。

#### 更新过程

##### change buffer

当需要更新一个数据页时，如果数据页在内存中就**直接更新**，而如果这个数据页还**没有在内**存中的话，在不影响数据一致性的前提下，InnoDB 会将这些**更新操作缓存**在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次**查询需要访问这个数据页**的时候，将数据页读入内存，然后**执行** change buffer 中与**这个页有关**的操作。通过这种方式就能保证这个数据逻辑的正确性。

需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，**change buffer 在内存中有拷贝，也会被写入到磁盘上**。

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束，必须将数据页读入到内存中判断，数据页在内存中了，直接更新内存就行了。唯一索引的更新就不能使用 change buffer，实际上也只有**普通索引可以使用**

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以**对更新性能的提升**是会很明显的。

对于要更新的**目标页不在**内存中。这时，InnoDB 的处理流程如下：

对于**唯一索引**来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；

对于**普通索引**来说，则是将更新记录在 change buffer，语句执行就结束了。

##### change buffer 的使用场景

因此，对于**写多读少**的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是**账单类、日志类**的系统。

假设一个业务的更新模式是写入之后马上会做查询，那么先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程，反而起到了反作用。

考虑到更新性能的影响，建议你尽量选择**普通索引**。

merge 的执行流程是这样的：

1、从磁盘读入数据页到内存（老版本的数据页）；

2、从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；

3、写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。

**redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。**

#### 问题

change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？

答案：虽然是只更新内存，但是在事务提交的时候，我们把 **change buffer 的操作也记录到 redo log** 里了，所以崩溃恢复的时候，change buffer 也能找回来。

### 2、怎么给字符串字段加索引

#### 如何给邮箱字段加索引

使用**前缀索引**，**定义好长**度，就可以做到既节省空间，又不用额外增加太多的查询成本。

```mysql
mysql> alter table SUser add index index1(email);
或
mysql> alter table SUser add index index2(email(6));
```

实际上，我们在建立索引时关注的是**区分度**，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。

```mysql
mysql> select count(distinct email) as L from SUser;

mysql> select 
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;
```

> 当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。

#### 前缀索引对覆盖索引的影响

如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用**覆盖索引**，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。

而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。

使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。

#### 其他方式（身份证号加索引）

第一种方式是使用倒序存储。

如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：

```mysql
mysql> select field_list from t where id_card = reverse('input_id_card_string');
```

由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。当然了，实践中你不要忘记使用 count(distinct) 方法去做个验证。

第二种方式是使用 hash 字段。你可以在表上再创建一个整数字段，来保存身份证的**校验码**，同时在这个字段上创建索引。

```mysql
mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
```

然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过 crc32() 函数得到的结果可能是相同的，所以你的查询语句 where 部分要判断 id_card 的值是否精确相同。

```mysql
mysql> select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string'
```

##### 使用倒序存储和使用 hash 字段这两种方法的异同点。

它们的相同点是，**都不支持范围查询**。倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样地，hash 字段的方式也只能支持等值查询。

它们的区别，主要体现在以下三个方面：

1、从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。

2、在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。

3、从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。

### 3、MySQL会“抖”一下？--偶尔SQL语句执行特别慢

InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作，当然也更新了内存。这个日志叫作 redo log（重做日志）

把内存里的数据写入磁盘的过程，术语就是 flush。

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**

平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在**刷脏页（flush）**。

#### 什么情况会引发数据库的 flush 过程呢？

1、InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。把 checkpoint 位置从 CP 推进到 CP’，就需要将两个点之间的日志，对应的所有脏页都 flush 到磁盘上。（这种情况是InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住）

2、系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。（这种情况其实是常态）

3、MySQL 认为系统“空闲”的时候

4、MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上

刷脏页虽然是常态，但是出现以下这两种情况，都是会**明显影响性能**的：

1、一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；

2、日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。

#### InnoDB 刷脏页的控制策略

首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。这就要用到 **innodb_io_capacity** 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成**磁盘的 IOPS**。

参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。

避免MySQL“抖”一下，需要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。脏页比例是通过`Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total`得到的，具体的命令参考下面的代码：

```mysql
mysql> select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
select @a/@b;
```

在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延。在 InnoDB 中，`innodb_flush_neighbors `参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。

> 如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。

### 4、count(*) 为什么越来越慢

#### count(*) 的实现方式

1、MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；

2、InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

注意的是，这里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快

InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，**MySQL 优化器会找到最小的那棵树来遍历**。

**在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。**

#### 自己计数，不使用count(*)

**数据库保存计数**:我们把这个计数直接放到数据库里单独的一张计数表C中

#### 不同的 count 用法

**count() 的语**义：count() 是一个**聚合函数**，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。

所以，`count(*)`、`count(主键 id)` 和 `count(1) `都表示返回**满足条件**的结果集的**总行数**；而 `count(字段）`，则表示返回满足条件的数据行里面，参数“**字段”不为 NULL 的总个数**。（count(列名)会过滤为null的情况，所以相同条件的查询下，`count(*)`的数量一定是大于等于count(列名)的。）

**对于 count(主键 id) 来说**，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。

**对于 count(1) 来说**，InnoDB 引擎遍历整张表，但**不取值**。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

> 单看这两个用法的差别的话，`count(1) `执行得要比 `count(主键 id) `**快**。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。

**对于 count(字段) 来说**：1、如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；2、如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。

**但是`count(*)`是例外**，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。

所以结论是：按照效率排序的话，count(字段)<count(主键id)<count(1)≈count(*)，我建议你，尽量使用 `count(*)`。(count(id)可能会选择最小的索引来遍历,而count(字段)的话，如果字段上没有索引，就只能选主键索引)

### 5、 “order by”是怎么工作的？

全字段排序 VS rowid 排序

如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。

如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。

这也就体现了 MySQL 的一个设计思想：**如果内存够，就要多利用内存，尽量减少磁盘访问**。

对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。

如果能够保证从索引上取出来的行，天然就是按照递增排序的话，是不是就可以不用再排序了呢？确实是这样，可以使用联合索引（**联合索引可以保证索引中第一个字段相同的情况下，第二个字段的值是有序的**）

### 6、如何正确的显示随机消息

如果直接使用` order by rand()`，这个语句需要 Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候你要尽量避开这种写法。

#### 随机取 3 个值

1、取得整个表的行数，记为 C；

2、取得 Y = floor(C * rand())。 floor 函数在这里的作用，就是取整数部分，得到 Y1、Y2、Y3；

3、再执行三个 limit Y, 1 语句得到三行数据。

```mysql
mysql> select count(*) into @C from t;
set @Y1 = floor(@C * rand());
set @Y2 = floor(@C * rand());
set @Y3 = floor(@C * rand());
select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行
select * from t limit @Y2，1；
select * from t limit @Y3，1；
```

**进一步优化：**

取 Y1、Y2 和 Y3 里面最大的一个数，记为 M，最小的一个数记为 N，然后执行下面这条 SQL 语句：

```mysql
mysql> select * from t limit N, M-N+1;
```

加上取整个表总行数的 C 行，这个方案的扫描行数总共只需要 C+M+1 行。

### 7、对索引字段做函数操作的坏处

**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**

这个函数操作也包含另外两种：**隐式类型转换**，**隐式字符编码转换**

> 优化器并不是要放弃使用这个索引，只是放弃了树搜索，还是有可能在这个索引上进行全索引扫描的由于加了函数操作，MySQL 无法再使用**索引快速定位**功能，而只能使用全索引扫描。

#### 类型转换

在 MySQL 中，字符串和数字做比较的话，是将**字符串转换成数字**。

```mysql
mysql> select * from tradelog where tradeid=110717; 
//如果tradeid是varchar，就会导致类型变换
//相当于
mysql> select * from tradelog where  CAST(tradid AS signed int) = 110717;
```

#### 字符编码转换

字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。

> 这个设定很好理解，utf8mb4 是 utf8 的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是**“按数据长度增加的方向**”进行转换的。

优化方法：1、统一修改字段的字符集；2优化SQL语句

```mysql
mysql> select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 
```

### 8、幻读--间隙锁

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

1、在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，**幻读在“当前读”**下才会出现。

2、别的事务中的更新语句导致在本事务中看到没有看到的行，不能称为幻读。**幻读仅专指“新插入的行”。**

#### 如何解决幻读？

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是**间隙锁 (Gap Lock)**。在语句执行中，在一行行扫描的过程中，**不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁**。

**行锁，分成读锁和写锁**。下图就是这两种类型行锁的冲突关系。

![c435c765556c0f3735a6eda0779ff151](http://ipic.jsgsjz.cn/2021-02-07-013809.png)

但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中**插入一个记录”这个操作**。间**隙锁之间都不存在冲突关系**。

间隙锁和行锁合称 next-key lock，间隙锁记为开区间，每个 next-key lock 是前开后闭区间。

**间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。**

间隙锁是在**可重复读**隔离级别下才会生效的。所以，你如果把隔离级别设置为**读提交**的话，就**没有间隙锁**了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。

### 9、Join使用和优化

在 join 语句执行过程中，**驱动表是走全表扫描，而被驱动表是走树搜索**。

1、使用 join 语句，性能比强行拆成多个单表执行 SQL 语句的性能要好；

2、如果使用 join 语句的话，需要让小表做驱动表。

但是，你需要注意，这个结论的前提是“可以使用被驱动表的索引”。

在决定哪个表做驱动表的时候，**应该是两个表按照各自的条件过滤**，过滤完成之后，计算参与 join 的各个字段的总数据量，**数据量小的那个表，就是“小表”**，应该作为驱动表。

### 10、加锁规则

> MySQL 后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，即 5.x 系列 <=5.7.24，8.0 系列 <=8.0.13。

#### **两个“原则”、两个“优化”和一个“bug”。**

1、原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。

2、原则 2：查找过程中访问到的对象才会加锁。

3、优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。

4、优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。

5、一个 bug：**唯一索引**上的范围查询会访问到不满足条件的第一个值为止。（此bug在mysql 8.0.18 已修复）

案例使用表：

```mysql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;

insert into t values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```

详细[案例](https://time.geekbang.org/column/article/75659)

[答疑一--日志索引相关](https://time.geekbang.org/column/article/73161)

[答疑二--锁相关](https://time.geekbang.org/column/article/78427)
