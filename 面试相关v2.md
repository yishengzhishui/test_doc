## 1.数组是可以比较的，切片是不可比较的。

因为切片是引用类型，它包含指向底层数组的指针、长度和容量。两个切片即使引用相同类型的底层数组，它们也不会被认为是相等的，因为它们的指针、长度和容量可能不同。

## 2.TCP的三次握手和四次挥手

TCP（传输控制协议）的三次握手和四次挥手是建立和断开连接的过程，确保数据的可靠传输。以下是对TCP三次握手和四次挥手的详细解释：

### TCP的三次握手（Three-Way Handshake）：

1. **第一步 - 客户端发送SYN（同步）请求：**

   - 客户端向服务器发送一个TCP报文，标志位设置为SYN（同步），并选择一个初始序列号（ISN）。
2. **第二步 - 服务器确认SYN并发送自己的SYN请求：**

   - 服务器收到客户端的SYN请求后，向客户端发送一个TCP报文，标志位设置为SYN和ACK（确认），同时选择自己的初始序列号。
3. **第三步 - 客户端确认服务器的SYN请求：**

   - 客户端收到服务器的SYN和ACK后，向服务器发送一个TCP报文，标志位设置为ACK，确认连接建立。

### TCP的四次挥手（Four-Way Handshake）：

1. **第一步 - 客户端发起关闭：**

   - 客户端向服务器发送一个TCP报文，标志位设置为FIN（结束）。
2. **第二步 - 服务器确认关闭请求：**

   - 服务器收到客户端的FIN后，发送一个TCP报文，标志位设置为ACK，确认收到关闭请求。
3. **第三步 - 服务器发起关闭：**

   - 服务器向客户端发送一个TCP报文，标志位设置为FIN，表示服务器准备关闭连接。
4. **第四步 - 客户端确认关闭：**

   - 客户端收到服务器的FIN后，发送一个TCP报文，标志位设置为ACK，表示客户端确认关闭。

通过这个过程，TCP连接被优雅地关闭。三次握手确保连接的建立，而四次挥手确保连接的安全关闭，双方都有机会完成未完成的数据传输。这种连接的建立和关闭过程是为了确保数据的可靠传输，以及释放网络资源。

## 3.事物的ACID

ACID 是数据库事务的四个关键属性，用于确保在事务执行过程中数据的一致性和可靠性。ACID 表示如下：

1. **原子性（Atomicity）：**

   - 原子性要求事务是不可分割的操作单元，要么完全执行，要么完全不执行。如果事务的任何部分失败，整个事务都会被回滚到原始状态，不会产生部分完成的结果。
2. **一致性（Consistency）：**

   - 一致性确保事务执行后，数据库从一个一致的状态转移到另一个一致的状态。事务的执行不能破坏数据库的完整性约束，例如唯一键、外键等。即使在事务执行期间发生错误，数据库也应该恢复到一致的状态。
3. **隔离性（Isolation）：**

   - 隔离性定义了事务之间的独立性，即一个事务的执行不应该受到其他并发事务的影响。事务的隔离性防止了并发执行时可能发生的问题，如脏读、不可重复读和幻读。数据库系统通过实现不同的隔离级别来提供不同程度的隔离性。
4. **持久性（Durability）：**

   - 持久性确保一旦事务被提交，其结果就会永久保存在数据库中，即使系统发生故障或崩溃。数据库系统通过将事务的日志记录到稳定的存储介质（如磁盘）来实现持久性。

这四个属性共同确保了事务的可靠性和稳定性。ACID 属性是数据库系统中保障数据一致性和完整性的基本原则，对于涉及关键业务数据的应用非常重要。

## 4.跨域问题

协议、域名和端口，任意一个不同就是跨域请求

在 Gin 框架中，要使用 CORS 中间件来解决跨域问题，你可以使用 `github.com/gin-contrib/cors` 包。设置`AllowHeaders`、`AllowOrigin`等。

```go
func corsHandle() gin.HandlerFunc {
	return cors.New(cors.Config{
		AllowHeaders: []string{"Content-Type", "Authorization"},
		// 你不加这个，前端是拿不到的
		ExposeHeaders: []string{"x-jwt-token", "x-refresh-token"},
		// 是否允许你带 cookie 之类的东西
		AllowCredentials: true,
		AllowOriginFunc: func(origin string) bool {
			if strings.HasPrefix(origin, "http://localhost") {
				// 你的开发环境
				return true
			}
			return strings.Contains(origin, "yourcompany.com")
		},
		MaxAge: 12 * time.Hour,
	})
}

```

## 5.Cookie 和Session

Cookie 是一种小型的文本文件，由服务器发送给用户的浏览器，并存储在用户的计算机上。

它包含了与用户相关的信息，比如用户的身份认证、会话数据等。Cookie 主要用于在浏览器和服务器之间保持状态，以实现跟踪用户行为、记录用户偏好等功能。当用户访问同一站点时，浏览器会将相关的 Cookie 信息附加到请求头中，从而向服务器提供存储在本地的信息。

Session 是服务器端用于存储用户信息的一种机制。与 Cookie 不同，Session 数据存储在服务器上，而不是在用户的浏览器中。当用户访问服务器时，服务器会为每个用户创建一个唯一的 Session，然后将 Session ID 发送给客户端，通常通过 Cookie 进行存储。客户端的 Cookie 中包含了 Session ID，而实际的用户数据则存储在服务器上。

## 6.go的recover

在 Go 语言中，`recover` 是一个内建函数，用于捕获在 `defer` 语句中由 `panic` 引发的运行时恐慌。`recover` 可以阻止 panic 继续传播，允许程序继续执行而不中断。可以实现类似php的try catch。

在 Go 中，可以通过使用 `recover` 来捕获 `panic`，从而避免程序因 `panic` 而崩溃。但需要注意的是，`recover` 只有在 `defer` 函数中调用才有效。

## 7.GO的GMP模型

Go 的 GMP 模型是指 Goroutine（协程）、M（线程管理器）、P（处理器）这三者之间的协作模型。这个模型是 Go 语言调度器实现并发的基础。以下是 GMP 模型的简述：

1. **Goroutine（协程）：**

   - Goroutine 是 Go 语言并发的基本单位，它是轻量级的用户态线程。
   - 每个 Goroutine 都有一个独立的执行栈，占用的内存相对较小。
   - 在程序执行过程中，可以创建成千上万个 Goroutine，而不会显著影响系统性能。
2. **M（线程管理器）：**

   - M 是 Go 语言中的线程管理器，负责管理和调度 Goroutine。
   - 在多核机器上，M 会与物理处理器一一对应。一个物理处理器上可以有多个 M，每个 M 负责管理一组 Goroutine。
   - M 会在需要的时候创建或销毁内核线程，以确保 Goroutine 能够被适当地并发执行。
3. **P（处理器）：**

   - P 是调度器的逻辑处理器，用于将 Goroutine 分配给 M 执行。
   - P 的数量可以在运行时调整，以适应系统的负载。
   - P 会维护一个 Goroutine 队列，当某个 M 空闲时，会从队列中获取 Goroutine 分配给 M 执行。
4. **GMP 之间的关系：**

   - M 和 P 是多对多的关系，即一个 M 可以和多个 P 关联，一个 P 也可以和多个 M 关联。
   - 当一个 Goroutine 被创建时，调度器会将其放入某个 P 的队列中。
   - 当一个 M 变得空闲时，它会从与之关联的 P 的队列中获取 Goroutine 执行。

GMP 模型的优势在于它充分利用了多核处理器的性能，并且在 Goroutine 的调度上实现了高度的灵活性。这种模型使得 Go 语言能够在高并发的情况下高效地运行，并且对于并发编程提供了简单而强大的抽象。

## 8.click house数据存储结构及主要特点

ClickHouse 是一个开源的列式数据库管理系统，它采用了列式存储结构，这种存储结构与传统的行式数据库存储结构有所不同。以下是 ClickHouse 数据存储结构的主要特点：

1. **列式存储：**

   - ClickHouse 是一种列式存储数据库，数据按列存储而非按行存储。每个列都存储一个属性的数据，相同列的数据是连续存储的。
   - 这种存储方式在分析性质的查询中表现出色，因为它允许跳过不必要的列，只读取需要的列，从而提高查询性能。
2. **数据压缩：**

   - ClickHouse 使用了多种压缩算法，以减小存储占用和提高查询性能。常见的压缩算法包括 LZ4、ZSTD、Delta、T64 等。
   - 压缩有助于减小磁盘 I/O、提高数据传输效率，并降低存储成本。
3. **MergeTree 表引擎：**

   - ClickHouse 中最常用的表引擎是 MergeTree，它适用于处理大量的时间序列数据。
   - MergeTree 表引擎支持按照指定的列对数据进行排序，并定期合并小的块以提高查询性能。
4. **数据划分：**

   - ClickHouse 支持水平分区和垂直切分，以便更好地管理大规模的数据。
   - 水平分区允许将表分成若干部分，每个部分称为分区，每个分区可以单独存储在不同的服务器上。
   - 垂直切分则是将一张表按列进行分割，将不同的列存储在不同的表中，以提高查询效率。
5. **BloomFilter：**

   - ClickHouse 使用 BloomFilter 用于快速过滤掉不存在于某个条件下的数据，以减小查询的范围。

这些特点使 ClickHouse 在大规模数据分析和查询场景中表现优越，特别适用于需要快速查询大量历史数据的应用。了解 ClickHouse 的存储结构对于进行性能调优、数据模型设计以及数据仓库的搭建都是重要的。

## 9.clickhouse vs mysql

ClickHouse 和 MySQL 是两种不同类型的数据库系统，它们在很多方面有着不同的设计目标和应用场景。以下是 ClickHouse 和 MySQL 的一些对比：

1. **数据模型：**

   - **ClickHouse：** ClickHouse 是一种列式存储数据库，适用于分析型查询。它以高性能和高压缩比为目标，特别擅长处理大量数据的分析和聚合操作。
   - **MySQL：** MySQL 是一种行式存储数据库，适用于事务性应用和联机事务处理（OLTP）场景。它更适合处理频繁的读写操作。
2. **查询性能：**

   - **ClickHouse：** ClickHouse 在大规模数据分析查询上表现出色，可以在秒级别的时间内完成复杂的聚合查询。但对于事务性操作，性能可能相对较低。
   - **MySQL：** MySQL 在事务性操作上表现良好，适用于需要频繁进行插入、更新和删除操作的应用。但在大规模数据分析场景下，性能可能不如 ClickHouse。
3. **索引：**

   - **ClickHouse：** ClickHouse 的索引设计主要基于 MergeTree 引擎，支持对列的多级索引，适用于大规模数据的范围查询。
   - **MySQL：** MySQL 支持多种索引类型，包括 B-Tree、Hash、Full-Text 等，可以更灵活地满足不同查询需求。
4. **事务支持：**

   - **ClickHouse：** ClickHouse 对事务支持有限，通常用于批处理和分析型查询，不适合处理复杂的事务操作。
   - **MySQL：** MySQL 提供强大的事务支持，可以满足需要 ACID 特性的应用场景，例如电子商务和金融领域。
5. **数据压缩：**

   - **ClickHouse：** ClickHouse 使用列式存储和多种压缩算法，以减小存储占用空间，并提高查询性能。
   - **MySQL：** MySQL 也支持数据压缩，但通常在 InnoDB 存储引擎中使用，以节省磁盘空间。
6. **分布式架构：**

   - **ClickHouse：** ClickHouse 提供了分布式架构，可以横向扩展以处理更大规模的数据。
   - **MySQL：** MySQL 也支持主从复制和分片，但在处理大规模数据时需要谨慎设计和管理。
7. **社区和生态系统：**

   - **ClickHouse：** ClickHouse 的社区相对较小，但在大数据分析领域逐渐得到了认可。
   - **MySQL：** MySQL 有着庞大的社区和生态系统，广泛用于各种应用场景。

综合考虑业务需求，选择 ClickHouse 还是 MySQL 取决于具体的使用场景和性能要求。 ClickHouse 更适合大规模数据分析，而 MySQL 更适合事务性应用。

10. redis 异步延时队列

Redis 延迟队列是一种利用 Redis 数据库实现的队列，其中的消息在一定的延迟时间之后才会被消费。延迟队列常用于处理具有延迟触发需求的任务，例如定时任务、重试机制等。

以下是实现 Redis 延迟队列的一种常见方式：

1. **使用有序集合（Sorted Set）：**

   - Redis 的有序集合是一个有序的字符串集合，其中每个字符串都关联着一个分数。延迟队列可以使用有序集合来存储消息，其中消息的分数表示消息的到期时间。
2. **将消息加入有序集合：**

   - 当有一个新的消息要加入队列时，将消息的内容作为字符串存储在有序集合中，同时使用当前的时间戳加上延迟时间作为消息的分数。这样，消息就会根据到期时间被添加到有序集合中。

   ```bash
   ZADD delay_queue <timestamp + delay> <message>
   ```
3. **定时轮询获取到期消息：**

   - 客户端或者后台任务可以定时轮询有序集合，查找到期的消息。通过比较当前时间戳和消息的到期时间，可以找到需要处理的消息。

   ```bash
   ZRANGEBYSCORE delay_queue 0 <current_timestamp> WITHSCORES
   ```
4. **处理到期消息：**

   - 获取到期的消息后，进行相应的处理，可以是执行任务、触发事件等。处理完成后，将消息从有序集合中移除。

   ```bash
   ZREM delay_queue <message>
   ```

通过上述方式，可以实现一个简单的 Redis 延迟队列。需要注意的是，这是一种基本的实现方式，具体根据业务需求和性能要求可以进行更复杂的优化和扩展。例如，可以考虑使用 Lua 脚本来原子地处理添加和移除操作，以保证操作的一致性。

## 10.进程、线程和协程

进程（Process）、线程（Thread）和协程（Coroutine）是计算机科学中用于实现并发执行的三种基本的执行单元。

**线程是独立调度的基本单位，进程是资源拥有的基本单位** 。

它们有着不同的特性和用途：

1. **进程（Process）：**

   - 进程是操作系统中的一个独立的执行环境。每个进程都有自己独立的地址空间、内存、文件描述符等资源，进程之间通常通过进程间通信（Inter-Process Communication，IPC）来进行数据交换。由于进程拥有独立的资源，进程之间的隔离性很强，但进程的创建和切换开销相对较大。
2. **线程（Thread）：**

   - 线程是进程中的一个执行单元，多个线程共享同一个进程的资源，包括地址空间、文件描述符等。线程的创建和切换开销相对较小，因为它们共享进程的资源。但多线程之间需要考虑同步和互斥，以避免竞态条件和数据访问冲突。
3. **协程（Coroutine）：**

   - 协程是一种轻量级的线程，它在用户空间中进行调度，不依赖于操作系统的线程和进程。协程之间可以通过协作式调度进行切换，而不是被操作系统强制切换。协程的切换开销相对较小，适用于高并发的场景。协程通常由开发者显式地调度，例如通过 yield 或 await 操作。

这三者的主要区别可以总结如下：

- 进程是独立的执行环境，拥有独立的资源，进程之间通常通过 IPC 进行通信。
- 线程是进程中的执行单元，多个线程共享同一个进程的资源，需要考虑同步和互斥。
- 协程是一种轻量级的线程，由用户空间进行调度，切换开销相对较小，适用于高并发场景。

## 11.linux常用命令

在 Linux 系统中，有许多强大而实用的命令，以下是一些常用的命令，它们可以帮助你查看系统信息、文件内容、进程状态等：

1. **系统信息：**

   - `uname -a`：显示系统信息，包括内核版本、操作系统等。
   - `hostname`：显示主机名。
   - `uptime`：显示系统的运行时间和平均负载。
2. **文件内容查看：**

   - `cat`：显示整个文件的内容。
   - `head`：显示文件的前几行。
   - `tail`：显示文件的后几行。
   - `less` 或 `more`：逐页查看文件内容。
3. **文本搜索和过滤：**

   - `grep`：在文件中搜索指定模式。
   - `find`：在文件系统中查找文件。
   - `awk`：用于文本处理，特别适合数据抽取和报告生成。
   - `sed`：流编辑器，用于文本替换和转换。
4. **进程管理：**

   - `ps`：显示当前进程状态。
   - `top`：实时显示系统资源使用情况和进程信息。
   - `kill`：发送信号给进程，用于终止或操作进程。
5. **网络相关：**

   - `ifconfig` 或 `ip`：显示网络接口信息。
   - `ping`：测试网络连接。
   - `netstat`：显示网络状态和连接信息。
   - `traceroute`：跟踪数据包的路径。
6. **用户和权限：**

   - `who`：显示当前登录用户。
   - `w`：显示当前登录用户及其活动。
   - `id`：显示用户的 UID 和 GID。
   - `chmod`：修改文件权限。
7. **系统日志：**

   - `dmesg`：显示内核日志。
   - `journalctl`：显示系统日志。

这只是一小部分常用的 Linux 命令，Linux 系统提供了丰富的命令行工具，可以满足不同的系统管理和开发需求。可以使用 `man` 命令查看命令的手册页，例如 `man ls`。

## 12.Redis批量查询

在 Redis 中，有多种命令可以用于批量查询数据，其中最常见的是 `MGET` 命令。以下是一些批量查询数据的命令：

1. **MGET 命令：**

   - `MGET` 命令用于获取多个键的值。通过指定多个键，可以一次性获取它们的值。

   ```bash
   MGET key1 key2 key3
   ```
2. **Pipeline 批量查询：**

   - 使用 Redis Pipeline 可以在一次请求中发送多个命令，并一次性获取它们的响应。这对于批量查询来说是一种高效的方式。

   ```bash
   MULTI
   GET key1
   GET key2
   GET key3
   EXEC
   ```

   这个示例中，`MULTI` 开启了一个事务，然后在事务中执行了三个 `GET` 命令，最后通过 `EXEC` 提交事务并获取结果。

## 13.浏览器和服务器连接，有哪些步骤

当浏览器与服务器建立连接时，通常经过以下步骤：

1. **DNS 解析：**

   - 当用户在浏览器中输入一个网址时，首先进行 DNS 解析，将域名解析为相应的 IP 地址。这一步骤由本地 DNS 缓存和 DNS 服务器完成。
2. **建立 TCP 连接：**

   - 通过浏览器向服务器发起 TCP 连接请求。这个过程通常经历三次握手，即客户端发送 SYN（同步）请求，服务器回应 SYN-ACK，最后客户端发送 ACK，建立起双向的 TCP 连接。
3. **发起 HTTP 请求：**

   - 一旦建立了 TCP 连接，浏览器就会发送一个 HTTP 请求给服务器。请求中包含用户所需的资源路径、请求方法（GET、POST 等）、头部信息等。
4. **服务器处理请求：**

   - 服务器接收到 HTTP 请求后，会根据请求的内容和服务器上的相应处理逻辑，生成并返回 HTTP 响应。
5. **服务器响应：**

   - 服务器向浏览器发送 HTTP 响应，响应中包含状态码、响应头（Content-Type、Content-Length 等）以及实际的响应体，可能是 HTML、CSS、JavaScript 文件等。
6. **浏览器解析响应：**

   - 浏览器接收到响应后，开始解析响应内容。对于 HTML 内容，浏览器会构建 DOM 树；对于 CSS 文件，浏览器会构建样式表；对于 JavaScript，浏览器会执行脚本。
7. **显示页面：**

   - 最后，浏览器根据解析后的内容渲染页面，呈现给用户。这包括将 HTML 内容显示在屏幕上，应用样式表，执行 JavaScript 以使页面交互。
8. **断开连接：**

   - 一旦页面被完全加载，或者用户离开当前页面，浏览器可能会断开与服务器的 TCP 连接。这个过程通常经历四次挥手，即客户端发送 FIN（结束）请求，服务器回应 ACK，然后服务器发送 FIN，客户端回应 ACK，完成断开连接。

这些步骤构成了浏览器与服务器之间的基本通信过程，被称为 HTTP 请求-响应模型。在这个过程中，HTTP 协议负责定义客户端和服务器之间的通信规则。

## 14.mysql主从同步的原理

MySQL 主从同步（Master-Slave Replication）是一种数据复制机制，它允许将一个 MySQL 主数据库的更改同步到一个或多个从数据库。主从同步的原理可以简要概括为以下几个步骤：

1. **二进制日志（Binary Log）的启用：**

   - 在主数据库上启用二进制日志，主要通过配置文件中的参数 `log-bin` 来实现。二进制日志记录了主数据库上的所有更改操作，包括插入、更新和删除。
2. **主数据库的更改事件记录：**

   - 当在主数据库上执行一条更改操作时（例如，插入一条记录），该操作会被记录到二进制日志中。每个更改都被赋予一个唯一的事件编号。
3. **从数据库连接到主数据库：**

   - 从数据库通过配置文件中的参数（如 `master-host`、`master-port`、`master-user`、`master-password`）配置连接到主数据库。
4. **从数据库获取主数据库的数据快照：**

   - 从数据库通过执行 `SHOW MASTER STATUS` 获取主数据库当前的二进制日志文件名和位置（position）。然后，从数据库通过 `CHANGE MASTER TO` 命令设置自己的复制位置，以获取主数据库的数据快照。
5. **从数据库开始复制：**

   - 从数据库执行 `START SLAVE` 命令，开始从主数据库复制数据。从此刻起，从数据库会不断地连接到主数据库，获取并应用主数据库的二进制日志中的更改。
6. **主从同步的实时复制：**

   - 一旦从数据库完成数据快照的复制，它会保持与主数据库的实时同步。主数据库上的每个更改都会被记录到二进制日志中，并通过复制线程传递给从数据库。
7. **监控主从同步状态：**

   - 可以通过执行 `SHOW SLAVE STATUS` 查看从数据库的同步状态。该命令提供了一系列的参数和状态信息，包括复制的位置、延迟等。
8. **故障处理和恢复：**

   - 如果发生主从同步的中断，可以通过重新配置从数据库的连接参数，获取主数据库的最新数据快照，然后继续复制。

主从同步的机制允许从数据库可以在主数据库上执行只读查询，从而分担主数据库的负载。这也提供了一种数据备份和故障恢复的机制。需要注意的是，主从同步并不保证主从数据库的完全一致性，因为在复制的过程中可能会出现网络延迟或其他因素。

## 15. MySql 内置加密函数

1. **AES 加密和解密：**

   - MySQL 提供了 `AES_ENCRYPT()` 和 `AES_DECRYPT()` 函数，用于进行基于AES的对称加密和解密。

   ```sql
   -- 使用AES加密
   SELECT AES_ENCRYPT('Hello, MySQL', 'secret_key');

   -- 使用AES解密
   SELECT AES_DECRYPT('encrypted_data', 'secret_key');
   ```
2. **MD5 和 SHA 加密：**

   - MySQL 提供了 `MD5()` 和 `SHA1()` 等函数，用于计算给定字符串的 MD5 或 SHA 散列值。

   ```sql
   SELECT MD5('password');
   SELECT SHA1('password');
   ```

   注意：MD5 和 SHA1 是单向散列函数，不可逆。

## 16.systemd 启用流程

1. **创建服务配置文件：**

* 在 `/etc/systemd/system/` 目录中创建以 `.service` 结尾的服务配置文件，例如 `my-service.service`。

2. 边界相关配置信息
3. 加载并启用

常用命令

```shell
# 启动一个服务
systemctl start serviceName

# 停止一个服务
systemctl stop serviceName

# 重启一个服务
systemctl restart serviceName

# 查看服务状态
systemctl status serviceName

# 启用服务（开机自启动）
systemctl enable serviceName

# 禁用服务（开机不自启动）
systemctl disable serviceName

# 查看系统日志
journalctl

# 查看指定服务的日志
journalctl -u serviceName

```

# MySql 汇总

## 1.索引的数据结构 B+树

B+树是一种多叉树，一棵 m 阶的 B+ 树定义如下：

1. 每个节点最多有 m 个子女。
2. 除根节点外，每个节点至少有 [m/2] 个子女，根节点至少有两个子女。
3. 有 k 个子女的节点必有 k 个关键字。

**叶子存放了数据，而非叶子节点只是存放了关键字。**

**叶子节点被链表串联起来了。**

优势：

1. B+ 树的高度和二叉树之类的比起来更低，树的高度代表了查询的耗时，所以查询性能更好。
2. B+ 树的叶子节点都被串联起来了，适合范围查询。
3. B+ 树的非叶子节点没有存放数据，所以适合放入内存中。

## 2.索引分类

• 聚簇索引和非聚簇索引：核心是叶子节点放的是数据本身，还是只是放了一个主键

• 联合索引（组合索引）和非联合索引：使用了多个列的就是联合索引

• 唯一索引和非唯一索引：

• 前缀索引：使用了列的一部分的索引。比如说在varchar(128) 的字段上，值利用前32个字符创建索引；

• 全文索引

• 覆盖索引：其实是指你查询的列，都是某个索引的列。覆盖索引最大的好处就是不用回表

## 3. 索引的代价

索引的代价索引并不是没有代价的，它会消耗很多的系统资源。

1. 索引本身需要存储起来，消耗磁盘空间。
2. 在运行的时候，索引会被加载到内存里面，消耗内存空间。
3. 在增删改的时候，数据库还需要同步维护索引，引入额外的消耗。

## 4.MySQL 为什么使用 B+ 树？

回答这个问题，你就不能仅仅局限在 B+ 树和 B 树上，你要连带着二叉树、红黑树、跳表一起讨论。总结起来，在用作索引的时候，其他数据结构都有一些难以容忍的缺陷。

1. 与 B+ 树相比，平衡二叉树、红黑树在同等数据量下，高度更高，性能更差，而且它们会频繁执行再平衡过程，来保证树形结构平衡。
2. 与 B+ 树相比，跳表在极端情况下会退化为链表，平衡性差，而数据库查询需要一个可预期的查询时间，并且跳表需要更多的内存。
3. 与 B+ 树相比，B 树的数据存储在全部节点中，对范围查询不友好。非叶子节点存储了数据，导致内存中难以放下全部非叶子节点。如果内存放不下非叶子节点，那么就意味着查询非叶子节点的时候都需要磁盘 IO。

## 5.为什么会不使用索引

1. 使用了 !=、LIKE 之类的查询。
2. 字段区分度不大。比如说你的 status 列只有 0 和 1 两个值，那么数据库也有可能不用。
3. 使用了特殊表达式，包括数学运算和函数调用。
4. 数据量太小，或者 MySQL 觉得全表扫描反而更快的时候。

## 6.EXPLAIN 关键字段

1. type：指的是查询到所需行的方式，从好到坏依次是 system > const > eq_ref > ref > range > index > ALL。

* system 和 const 都可以理解为数据库只会返回一行数据，所以查询时间是固定的。
* eq_ref 和 ref 字面意思是根据索引的值来查找。
* range：索引范围扫描。
* index：索引全表扫描，也就是扫描整棵索引。
* ALL：全表扫描，发起磁盘 IO 的那种全表扫描。

1. possible_keys：候选的索引。
2. key：实际使用的索引。
3. rows：扫描的行数。数据库可能扫描了很多行之后才找到你需要的数据。
4. filtered：查找到你所需的数据占 rows 的比例。

## 7.SQL优化

* (1）避免返回不需要的列，尽量使用覆盖索引避免回表
* (2)避免For循环里面查单条数据，改为一条sql查集合。
* (3)建表的时候考虑增加冗余字段，尽可能保持单表查询，而非多表Join.
* (4)在所有的排序场景中，都应该尽量利用索引来排序.
* (5)算count行数的时候，如果业务场景要求不高，可以有一个偏门方法，就是执行explain select * from t where xxxx，在执行计划里面会预估出来大致的行数。
* (6)where 替代 having

## 8.临键锁、间隙锁、行锁

在 MySQL 的 InnoDB 引擎里面，锁是借助索引来实现的。或者说，加锁锁住的其实是索引项，更加具体地来说，**就是锁住了叶子节点。**

1. **间隙锁（Gap Lock）：**

   - 间隙锁是锁定两个键之间的间隙，阻止其他事务在这个间隙内插入新的记录，从而解决幻读问题。但间隙锁并没有锁住具体的记录，而只是锁住了两个键之间的空隙。
   - 间隙锁是在**可重复读**隔离级别下才会生效的
2. **行锁（Row Lock）：**

   - 行锁是直接锁定某一行的记录，阻止其他事务对该行的修改。行锁是一种更精确的锁定方式，但在某些情况下可能导致并发性能下降。
3. **临键锁（Next-Key Lock）：**

   - 临键锁是 MySQL 在可重复读隔离级别下使用的一种锁，它实际上是间隙锁和行锁的结合。它锁定了一个范围，并且包括了具体的记录，以解决幻读问题。临键锁确保事务在读取数据时不受其他事务插入数据的干扰。
   - 它可以解决在可重复读隔离级别之下的幻读问题。

## 9. 加锁的规则

### **两个“原则”、两个“优化”和一个“bug”。**

1、原则 1：加锁的基本单位是 临键锁（next-key lock）。next-key lock 是前开后闭区间。

2、原则 2：查找过程中访问到的对象才会加锁。

3、优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。

4、优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。

5、一个 bug：**唯一索引**上的范围查询会访问到不满足条件的第一个值为止。（此bug在mysql 8.0.18 已修复）

## 10、锁的特性

在 InnoDB 引擎里面，锁是依赖于索引来实现的。

或者说，锁都是加在索引项上的。因此，如果一个查询用了索引，那么会用行锁，**如果没用到任何索引，那么就会用表锁**。

此外，在 MySQL 里面，间隙锁和临键锁是只工作在**可重复读**这个隔离级别下的。

## 11.临键锁引发的死锁

### 问题：

假设说现在数据库中 ID 最大的值是 78。那么如果两个业务进来，同时执行这个逻辑。一个准备插入 id=79 的数据，一个准备插入 id = 80 的数据。如果它们的执行时序如下图，那么你就会得到一个死锁错误。

在线程 1 执行 SELECT FOR UPDATE 的时候，因为 id=79 的数据不存在，所以实际上数据库会产生一个 (78，supremum] 的临键锁。

类似地，线程 2 也会产生一个 (78，supremum] 临键锁。

当线程 1 想要执行插入的时候，它想要获得 id = 79 的行锁。当线程 2 想要执行插入的时候，它想要获得 id = 80 的行锁，这个时候就会出现死锁。因为线程 1 和线程 2 同时还在等着对方释放掉持有的间隙锁。

### 解决：

方案1：使用行锁，从而规避了死锁问题

先插入一个默认的数据。如果没有数据，那么会插入成功；如果有数据，那么会出现主键冲突或者唯一索引冲突，插入失败。那么在插入成功的时候，执行以前数据不存在的逻辑，但是因为此时数据库中有数据，所以不会使用间隙锁。

方案2：放弃悲观锁，使用乐观锁。

```mysql
UPDATE xxx SET data = newData WHERE id = 1 AND data = oldData。
```

## 12.脏读，不可重复读，幻读

* 脏读是指读到了别的事务还没有提交的数据。之所以叫做“脏”读，就是因为未提交数据可能会被回滚掉。
* 不可重复读是指在一个事务执行过程中，对同一行数据读到的结果不同。
* 幻读是指在事务执行过程中，别的事务插入了新的数据并且提交了，然后事务在后续步骤中读到了这个新的数据。

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是**间隙锁 (Gap Lock)**。在语句执行中，在一行行扫描的过程中，**不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁**。

## 13. 为什么有了锁，还需要 MVCC

避免读写阻塞

1. **锁的机制：**
   * **悲观锁：** 基于锁的并发控制机制是一种悲观的机制，它认为并发访问会导致冲突。在事务执行期间，它会使用锁来限制其他事务对相同资源的访问，以确保事务的一致性。
   * **问题：** 悲观锁可能导致大量的阻塞和等待，尤其是在高并发环境下，因为多个事务可能需要等待对相同资源的独占访问。
2. **MVCC 的机制：**
   * **乐观锁：** MVCC 是一种乐观的并发控制机制，它假设并发访问不会导致冲突。每个事务在开始时读取数据的版本号，并在提交时比较版本号，以确定是否有其他事务对数据进行了更改。
   * **优势：** MVCC 可以提高并发性能，因为事务可以并行执行而无需互相阻塞。

## 14 MVCC

MVCC 是 MySQL InnoDB 引擎用于控制数据并发访问的协议。

MVCC 主要是借助于版本链来实现的。

在 InnoDB 引擎里面，每一行都有两个额外的列，一个是 trx_id，代表的是修改这一行数据的事务 ID。另外一个是 roll_ptr，代表的是回滚指针。

InnoDB 引擎通过回滚指针，将数据的不同版本串联在一起，也就是版本链。这些串联起来的历史版本，被放到了 undolog 里面。当某一个事务发起查询的时候，MVCC 会根据事务的隔离级别来生成不同的 Read View，从而控制事务查询最终得到的结果。

## 15.Read View

Read View 只用于已提交读和可重复读两个隔离级别，它用于这两个隔离级别的不同点就在于什么时候生成 Read View。

* 已提交读：事务每次发起查询的时候，都会重新创建一个新的 Read View。
* 可重复读：事务开始的时候，创建出 Read View。

## 16. redo log 和 bin log

这两种日志有以下三点不同。

1、redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

2、redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

3、redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，**并不会覆盖以前的日志**。

redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

## 17.redo log

有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面。

（InnoDB引擎先把记录写到redo log 中，redo log 在哪，他也是在磁盘上，这也是一个写磁盘的过程，但是与更新过程不一样的是，更新过程是在磁盘上随机IO，费时。 而写redo log 是在磁盘上顺序IO。效率要高。）

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。

## 18. undo log

MySQL的Undo Log（回滚日志）是一种用于实现事务的关键组件，它记录了事务执行过程中所做的修改，以便在事务回滚或数据库崩溃时进行数据的恢复。Undo Log 是实现数据库事务的ACID特性（原子性、一致性、隔离性、持久性）的重要机制之一。

以下是 Undo Log 的主要特点和作用：

1. **记录事务修改：** 当事务执行更新、插入或删除操作时，MySQL会在Undo Log中记录这些修改的详细信息，包括修改前的数据内容。
2. **用于回滚：** 如果事务需要回滚（Rollback），Undo Log中的信息可以用于撤销事务所做的修改，将数据还原到事务开始之前的状态。
3. **多版本并发控制（MVCC）：** Undo Log 也与多版本并发控制（MVCC）有关。在MVCC中，每个事务在修改数据时会创建一个新版本，而老版本的数据则通过Undo Log进行保留，以便其他事务在读取数据时可以看到一致的快照。
4. **事务的隔离性：** Undo Log 的使用有助于实现数据库事务的隔离性。当一个事务正在修改某个数据时，其他事务可以通过Undo Log来获取该数据的老版本，从而避免读取到未提交的数据。
5. **持久性保证：** Undo Log 的数据会持久保存在磁盘上，以确保在数据库崩溃或异常情况下，可以通过Undo Log进行数据的恢复。

总体来说，Undo Log 是 MySQL 中一项关键的机制，它确保了事务的一致性和隔离性，同时支持了事务的回滚和数据库的恢复。 Undo Log 的设计使得 MySQL 能够有效地处理并发事务和保障数据库的稳定性。

## 19.数据迁移方案

基本步骤。

1. 创建目标表。
2. 用源表的数据初始化目标表。
3. 执行一次校验，并且修复数据，此时用源表数据修复目标表数据。(有update_time字段，根据这个增量修复)
4. 业务代码开启双写，此时读源表，并且先写源表，数据以源表为准。
5. 开启增量校验和数据修复，保持一段时间。
6. 切换双写顺序，此时读目标表，并且先写目标表，数据以目标表为准。
7. 继续保持增量校验和数据修复。
8. 切换为目标表单写，读写都只操作目标表。

### mysqldump数据导入导出优化

1. 加快导出速度能做的事情并不多，主要就是开启 extended-insert 选项，将多行合并为一个 INSERT 语句。

```shell
  mysqldump --extended-insert -u your_username -p your_password your_database > dump.sql
```

2. 加快导入速度就可以做比较多的事情。

* 关闭唯一性检查和外键检查，源表已经保证了这两项，所以目标表并不需要检查。

```mysql
  mysql> SET foreign_key_checks = 0;
  mysql> SET unique_checks = 0;
```

* 关闭 binlog，毕竟导入数据用不着 binlog。

```mysql
  SET sql_log_bin = 0;
```

* 调整 redo log 的刷盘时机，把innodb_flush_log_at_trx_commit 设置为 0。

## 20索引创建规则

* 在 WHERE 条件里面经常出现的。比如说外键；
* 使用有很多不同值的列：所以类似于 Status 这种枚举的效果就不是很好；
* 不要使用很长的列：比如说 BLOB 这种，或者很长的 varchar。一定要用的话，创建前缀索引；

创建联合索引，确定索引的顺序：

• 选择性高的在前面；

• 经常用作范围查询（也就是会中断索引使用的）放在后面；
